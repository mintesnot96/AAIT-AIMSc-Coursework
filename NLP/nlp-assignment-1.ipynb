{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6927883,"sourceType":"datasetVersion","datasetId":3977779},{"sourceId":6946401,"sourceType":"datasetVersion","datasetId":3989408}],"dockerImageVersionId":30579,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"###  ADDIS ABABA UNIVERSITY \n    \n###  ADDIS ABABA INSTITUTE OF TECHNOLOGY\n    \n####  SCHOOL OF INFORMATION TECHNOLOGY AND ENGINEERING – SITE\n    \n####  Department of MSC in Artificial Intelligence                                           \n\n\n\n### Course Assignment       \n>>###### Course:  NLP\n>>###### Name:    Mintesnot Fikir\n>>###### Section: Regular \n>>###### IDs:   GSR1669/15   \n>>###### Submitted to: Fantahun B. (PhD)\n>>###### Submission date November 2023","metadata":{}},{"cell_type":"markdown","source":"### NLP Assignment-1 (12%) \nFantahun Bogale\n•\nOct 20 (Edited Nov 6)\n12 points\nDue Yesterday, 11:59 PM\n- This assignment has three (3) tasks each having its own weight. The total weight of this assignment will be 12%. \n- This assignment is based on one version of the Amharic Corpus (GPAC) created by this paper. https://doi.org/10.3390/info12010020. \n- You can access this corpus using this link: link. https://drive.google.com/file/d/1jEOAORylTBAiNKes_JerD6gFF7cnLFyr/view?usp=sharing\n- This assignment is an individual assignment. Experience sharing and discussion is possible, however each student is responsible to submit and defend his work. \n- Presentation may be required (get ready for that)\n>\n\nNote: The corpus may require a good size of RAM to load; hence you can make use of Google Collab, Kaggle or more if your device is incapable. \n>\nQuestions:\n\n#1 N-gram language model (6%)\n - 1.1 Create n-grams for n=1, 2, 3, 4. You can show sample prints. \n - 1.2 Calculate probabilities of n-grams and find the top 10 most likely n-grams for all n. \n - 1.3 What is the probability of the sentence. \"ኢትዮጵያ ታሪካዊ ሀገር ናት \". You can also try more sentences. \n - 1.4 Generate random sentences using n-grams; explain what happens as n-increases based on your output.\n >\n \n#2 Evaluate these Language Models Using Intrinsic Evaluation Method (3%)\n          \n#3 Evaluate these Language Models Using Extrinsic Evaluation Method (3%)\n       You can make use of any task convenient for you to evaluate the n-gram models you have created.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# Solution","metadata":{}},{"cell_type":"markdown","source":"<a name=\"0\"></a>\n# Outline\n[ Preprocessing the data](#0.1)\n\n[1. N-gram language mode ](#1)\n- [1.1. Create n-grams for n=1, 2, 3, 4. You can show sample prints. ](#1.1)\n- [1.2. Calculate probabilities of n-grams and find the top 10 most likely n-grams for all n. ](#1.2)\n- [1.3. What is the probability of the sentence. \"ኢትዮጵያ ታሪካዊ ሀገር ናት \". You can also try more sentences. ](#1.3)\n- [1.4. Generate random sentences using n-grams; explain what happens as n-increases based on your output](#1.4)\n\n\n[2. Evaluate these Language Models Using Intrinsic Evaluation Method](#2) <br>\n[3. Evaluate these Language Models Using Extrinsic Evaluation Method](#3)","metadata":{"execution":{"iopub.status.busy":"2023-11-11T21:47:42.690485Z","iopub.execute_input":"2023-11-11T21:47:42.691035Z","iopub.status.idle":"2023-11-11T21:47:42.706003Z","shell.execute_reply.started":"2023-11-11T21:47:42.690954Z","shell.execute_reply":"2023-11-11T21:47:42.704244Z"}}},{"cell_type":"markdown","source":"<a name=\"0.1\"></a>\n## Preprocessing  ","metadata":{}},{"cell_type":"markdown","source":"### Import necessary libraries","metadata":{}},{"cell_type":"code","source":"import pickle\nimport nltk\nimport random\nimport gc\nfrom nltk import ngrams\nfrom nltk.util import ngrams\nfrom nltk import FreqDist\nfrom collections import Counter\nimport random\nimport math\nimport time\nimport string","metadata":{"execution":{"iopub.status.busy":"2023-11-21T22:14:27.482816Z","iopub.execute_input":"2023-11-21T22:14:27.483202Z","iopub.status.idle":"2023-11-21T22:14:28.947234Z","shell.execute_reply.started":"2023-11-21T22:14:27.483170Z","shell.execute_reply":"2023-11-21T22:14:28.946196Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"### Reading the corpus from local folder(text file) as a text file","metadata":{}},{"cell_type":"code","source":"starttime = time.time()\nwith open('/kaggle/input/amharic-corpus-general/GPAC.txt', 'r', encoding='utf-8') as file:\n    text = file.read()\nendtime = time.time()\nprint(f\"Time taken for read the text as a string is :  {(endtime - starttime)/60}  minuets\")","metadata":{"execution":{"iopub.status.busy":"2023-11-21T22:14:28.949105Z","iopub.execute_input":"2023-11-21T22:14:28.950051Z","iopub.status.idle":"2023-11-21T22:14:49.658194Z","shell.execute_reply.started":"2023-11-21T22:14:28.950015Z","shell.execute_reply":"2023-11-21T22:14:49.656551Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Time taken for read the text as a string is :  0.3450124700864156  minuets\n","output_type":"stream"}]},{"cell_type":"code","source":"print(type(text))","metadata":{"execution":{"iopub.status.busy":"2023-11-21T22:14:49.659674Z","iopub.execute_input":"2023-11-21T22:14:49.660005Z","iopub.status.idle":"2023-11-21T22:14:49.667174Z","shell.execute_reply.started":"2023-11-21T22:14:49.659979Z","shell.execute_reply":"2023-11-21T22:14:49.665732Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"<class 'str'>\n","output_type":"stream"}]},{"cell_type":"markdown","source":"###### As we have imported text file as a single string, next step is to make it comfortable for farther processing, so we need to tokenize the string into list of tokens","metadata":{}},{"cell_type":"code","source":"print(\"\\n\\n Sample text data:\\n\\n\",text[4:3000])","metadata":{"execution":{"iopub.status.busy":"2023-11-21T22:14:49.670153Z","iopub.execute_input":"2023-11-21T22:14:49.670527Z","iopub.status.idle":"2023-11-21T22:14:49.682219Z","shell.execute_reply.started":"2023-11-21T22:14:49.670489Z","shell.execute_reply":"2023-11-21T22:14:49.680764Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"\n\n Sample text data:\n\n ምን መሰላችሁ? (አንባቢያን) ኢትዮጵያ በተደጋጋሚ ጥሪው ደርሷት ልትታደመው ያልቻለችው የአለም የእግር ኳስ ዋ ለ19ኛ ጊዜ በደቡብ አፍሪካ ሲጠጣ፣ በሩቅ እያየች አንጀቷ ባረረ ልክ በአመቱ በለስ ቀናትና ሌላ ዋ ልትታደም ሁለት ልጆቿን ወደ ደቡብ አፍሪካ ላከች፡፡6ኛው ቢግ ብራዘርስ አፍሪካ አብሮ የመኖር ውድድር በደቡብ አፍሪካ ተካሂዷል፡፡ ከተለያዩ 14 የአፍሪካ አገራት የተውጣጡ 26 ያህል ተሳታፊዎች የተካፈሉበት ይህ ውድድር፣ ግለሰቦች በፈታኝ ሁኔታ ውስጥ በማለፍ ብቃታቸውን የሚያስመሰክሩበት መሆኑን ሰምተናል፡፡ የሚገጥሟቸውን የተለያዩ ፈተናዎች በትእግስትና በጥበብ ማለፍ፣ ከሌሎች ጋር ተስማምቶ መዝለቅ፣ ችግሮችን በብልጠት መፍታት ወዘተ     በየጊዜው ከሚደረገው ቅነሳ ተርፈው ለ91 ቀናት ያህል በውድድሩ መቆየት የቻሉ ሁለት ተወዳዳሪዎች እያንዳንዳቸው 200 ሺህ ዶላር እንደሚሸለሙም ሲናገር ነበር፡፡ በዘንድሮው ውድድር አገራችን ዳኒ እና ሃኒ የተባሉ ሁለት ወጣቶችን ብታሰልፍም ዳኒ ቀደም ብሎ የቅነሳው ሰለባ ሲሆን ሃኒም በቅርቡ ከውድድር ውጭ ሆናለች፡፡ይህቺን የአገሪቱ ብቸኛ ተስፋ ወደ አሸናፊነት ለማሸጋገር የህዝብ የድጋፍ ድም ወሳኝ መሆኑን የተገነዘበው ወዳጄ ነው እንግዲህ                835  የሚል አገራዊ ጥሪ ያስተላለፈልኝ   ያኔ ሃኒ ከውድድሩ ከመሰናበቷ በፊት፡፡ወዳጄ የአገሩን ስም በአሸናፊነት የማስጠራት ከፍተኛ ጉጉት፣ አገሬ እንዳትሸነፍ የሚል ከፍተኛ ስጋት እንዳደረበት ይሰማኛል፡፡ ጉጉቱ ሳይሆን ስጋቱ የወዳጄን የዋህነት         ፡፡ሃኒም ኢትዮጵያም ይሸነፉ ይሆን? በሚል እንዲህ ከንቱ ስጋት የሚያንገበግባቸውን አገር ወዳድ ዜጐች እኔ የዋሆች እላቸዋለሁ፡፡የዋሆች ሆይ!አትስጉ    ስለ ሃኒም    ስለ ኢትዮጵያም አትስጉ፡፡ ውድድሩ ቢግ ብራዘርስ አፍሪካ በአርቴፊሻል ፈተናዎች ውስጥ አልፎ ለሦስት ወራት የመቆየት ውድድር ነው፡፡ ደቡብ አፍሪካ ለስድስተኛ ጊዜ ካዘጋጀችው ቢግ ብራዘርስ የመረረ ውድድር ለሺህ አመታት በተከታታይ ስታዘጋጅ የኖረች፣ መላ ህዝቧን አሳትፋ መላ ህዝቧን ስትሸልም የኖረች አገር ናት   ኢትዮጵያ!የደቡብ አፍሪካው እንጂ የኢትዮጵያው ቢግ ብራዘርስ ሶስት ወር ተብሎ ቀን የሚቆረጥለት ውስን የፈተና ጊዜ የለውም፡፡ አንዲት ኢትዮጵያ በአንዲት ሃኒ ሳይሆን በመላው ህዝቧ ነው የምትወከለው    ለምን ቢባል ሶስት ወር ሳይሆን ሶስት ሺህ ዘመን በራሷ ቢግ ብራዘርስ ተካፍላ የምንጊዜም አሸናፊ ሆና ዘልቃለችና፡፡ኢትዮጵያ ለዘመናት ባስተናገደችው የራሷ ቢግ ብራዘርስ አቻ የለሽ ፈተና ውስጥ መላው ህዝቧን እያሳተፈች ድሉን ከህዝቧ እጅ ባለማስነጠቅ ሃትሪክ የሰራች (በሺህ አመታት ስሌት) የምንጊዜም ድል ባለቤት እኮ ናት!የዋሆች    ስለ ደቡብ አፍሪካው ቢግ ብራዘር ስለምን ትጨነቃላችሁ?    ሃኒ እኮ ለእግር ኳስ አይደለም የሄደችው፡፡ እሱንማ ብለነው ብለነው አልሆን ብሎ ቸግሮናል፡፡ ደረጃችን ከሌሎች በታች ሆኖ ቀርቶብን በሩቅ እያየነው ተብሰልስለናል፡፡ አሁን ሃኒን ወደ ደቡብ አፍሪካ የላክናት ከአቅሟ በላይ ሳይሆን በታች ወርዳ ወደምትጫወትበት  አብሮ የመኖር ቀላል ፉክክር ነው፡፡ ሰቆቃንና ፈተናን ተጋፍጦ በመኖር የአለም ሻምፒዮናነቱን አለም በአንድ ድም ያፀደቀለት አሸናፊ ህዝብ ወኪል የሆነችው ሃኒ፣ ያለ ዲቪዚዮኗ ስንትና ስንት ቁልቁል ወርዳ እኮ ነው የተወዳደረችው፡፡ሃኒ ከቢግ ብራዘር ውድድር ውጭ መሆኗን ሰሞኑን ሰማሁ፡፡ ሰማሁና ሳቅኩ፡፡ ለምን ሳቅኩ? የአገሬ መሸነፍ የማያንገበግበኝ ሰው ሆኜ ነውን?    አይመስለኝም!ሃኒ ከውድድሩ ውጭ የሆነችው የቢግ ብራዘርስ አብሮ የመኖር ውድድር አሸንፏት ወይም አቅቷት አይመስለኝም! እሷ ከመስፈርቱ በላይ ሆና እንጂ!    (               እንደ ለት)   ኢትዮጵያ ከ6ኛው የቢግ ብራዘርስ አፍሪካ ውድድር የተሰናበተችው በቀላል ሚዛን ውድድር የከባድ ሚዛን ተወዳዳሪ በማሰለፏ ነው ባይ ነኝ፡፡ ልክ በእግር ኳስ ውድድር ላይ እንደሚከሰተው    ለምሳሌ ከ16 አመት በታች የሆኑ ተጫዋቾች በሚሳተፉበት የታዳጊ ወጣቶች ሻምፒዮና ላይ የ25 አመት እድሜ ያለው ተጫዋች በማሰለፉ ከውድድር ውጭ እንደሚሆን ክለብ፡፡ይመስለኛል    ሃኒ ከውድድሩ የተባረረችው የቢግ ብራዘርስ ፈተና ስላቃታት አይደለም፡፡ ምናልባትም እሷ ለፈተናው ከብዳው ቢሆን እንጂ፡፡ ሃኒ ከቢግ ብራዘር ግቢ ተባረረች የሚሉ፣ እነሱ አንዷን ሃኒ ብቻ የሚያዩ የዋሆች ናቸው፡፡ ሃኒ እዚያው ደቡብ አፍሪካ ናት    ደቡብ አፍሪካ    ኡጋንዳ    ኬኒያ    ሊቢያ    ሻሸመኔ    ቤሩት    አሜሪካ    እዚህም እዚያም ተበትና ከቢግ ብራዘር የመረረ ሰቆቃ ውስጥ እየተንገላታች ችግር ቻይነቷን እያስመሰከረች ያለች ብዙ ኢትዮጵያዊ ሴት ናት ሃኒ፡፡ቢግ ብራዘር በሚሉት የፌክ ፈተና እና ፎርጅድ ውጣ ውረድ ተሸነፋችሁ ሲሉን ሰማሁና ሳቅኩ!ዳኒ እና ሃኒ አይችሉም ተብለው መባረራቸው በሳቅ አፈረሰኝ፡፡ምን ማለታቸው ነው ዳኞቹ?    እንደ ዳኒ    እንደ ሃኒ    እንደ መላው ኢትዮጵያዊ ለችግር ሳይረታ ዘመናትን ያለፈ ማን ነው?    ተቻችሎ መኖር ከሆነ ጉዳዩ    ማን እንደነሱ ተቻቻይ አለና ነው!    ስድብ ዘለፋን አይደለም፣ ግርፋትን ችሎ የኖረ ቆዳው ድርብ ህዝብ እኮ ነው ተሸንፈሃል የተባለው፡፡ከዚህ በላይ ኮሜዲ አለ እንዴ?ለሶስት ሺህ ዘመን ክፉ ደጉን ችሎ አብሮ የኖረ ህዝብ እንዴት ነው ለሶስት ወር አብሮ መኖር አቃተህ ተብሎ ቀይ ካርድ የሚሰጠው?ሃኒ እና ዳኒ እኮ በቢ\n","output_type":"stream"}]},{"cell_type":"markdown","source":"\n##### The dataset contains numerous extraneous characters and symbols which require further processing. Below is a list of some of these:\n\n- Ethiopian punctuation marks: '፡', '።', '፣', '፤', '፥', '፦', '፧', '፨';\n- Lowercase letters: 'a' to 'z';\n- Uppercase letters: 'A' to 'Z';\n- Numerals: '0' to '9';\n- Special characters and symbols: '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\', ']', '^', '_', '`', '{', '|', '}', '~'.","metadata":{}},{"cell_type":"markdown","source":"### Tokenize the corpus\nTo facilitate more efficient processing of the corpus, the initial phase should involve tokenization. In this case, since the text comprises a mix of individual characters, symbols, and potential punctuation marks, tokenization would involve categorizing these elements into meaningful groups that can be separately identified and analyzed. \n\nBy breaking down the text into tokens, we can apply subsequent processing steps more effectively. These steps could include filtering out unnecessary symbols, converting characters to a uniform case for consistency, or identifying and preserving meaningful punctuation for further linguistic analysis. Tokenization simplifies these tasks by providing a structured approach to dissecting the text, allowing for a more manageable and targeted processing workflow.","metadata":{}},{"cell_type":"markdown","source":"we have 2 ways of tokenization either create Custom Tokenization Function from scrach or use third party library ","metadata":{}},{"cell_type":"code","source":"# tokenizer functions\nimport string\ndef tokenizer(text: str):\n    tokens = []\n    current_token = ''\n    \n    for char in text:\n        if char.isspace() or char in string.punctuation:\n            if current_token:\n                tokens.append(current_token)\n                current_token = ''\n        else:\n            current_token += char\n    \n    if current_token:\n        tokens.append(current_token)\n    \n    return tokens","metadata":{"execution":{"iopub.status.busy":"2023-11-21T22:14:49.683952Z","iopub.execute_input":"2023-11-21T22:14:49.684292Z","iopub.status.idle":"2023-11-21T22:14:49.696304Z","shell.execute_reply.started":"2023-11-21T22:14:49.684263Z","shell.execute_reply":"2023-11-21T22:14:49.694850Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"- The above custom tokenization function tokenizer iterates over each character in the input string. It builds tokens by appending characters until it encounters a whitespace or a punctuation character, at which point it adds the current token to the list and starts building the next one. This function does not use regular expressions and relies purely on sequential character checking, making it straightforward but potentially less efficient for large texts compared to regex-based solutions.","metadata":{}},{"cell_type":"code","source":"start_time = time.time()\ntokens = tokenizer(text)\nend_time = time.time()\nprint(f\"\\n\\nTime taken for tokenizetion:  {(end_time - start_time)/60}  minutes \\n\")\nprint(tokens[:200])\nprint('\\nSize of valid tokens are: ',len(tokens),'Words(tokens)')","metadata":{"execution":{"iopub.status.busy":"2023-11-21T22:14:49.698392Z","iopub.execute_input":"2023-11-21T22:14:49.699026Z","iopub.status.idle":"2023-11-21T22:17:07.656183Z","shell.execute_reply.started":"2023-11-21T22:14:49.698966Z","shell.execute_reply":"2023-11-21T22:17:07.654702Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"\n\nTime taken for tokenizetion:  2.299083085854848  minutes \n\n['ምን', 'መሰላችሁ', 'አንባቢያን', 'ኢትዮጵያ', 'በተደጋጋሚ', 'ጥሪው', 'ደርሷት', 'ልትታደመው', 'ያልቻለችው', 'የአለም', 'የእግር', 'ኳስ', 'ዋ', 'ለ19ኛ', 'ጊዜ', 'በደቡብ', 'አፍሪካ', 'ሲጠጣ፣', 'በሩቅ', 'እያየች', 'አንጀቷ', 'ባረረ', 'ልክ', 'በአመቱ', 'በለስ', 'ቀናትና', 'ሌላ', 'ዋ', 'ልትታደም', 'ሁለት', 'ልጆቿን', 'ወደ', 'ደቡብ', 'አፍሪካ', 'ላከች፡፡6ኛው', 'ቢግ', 'ብራዘርስ', 'አፍሪካ', 'አብሮ', 'የመኖር', 'ውድድር', 'በደቡብ', 'አፍሪካ', 'ተካሂዷል፡፡', 'ከተለያዩ', '14', 'የአፍሪካ', 'አገራት', 'የተውጣጡ', '26', 'ያህል', 'ተሳታፊዎች', 'የተካፈሉበት', 'ይህ', 'ውድድር፣', 'ግለሰቦች', 'በፈታኝ', 'ሁኔታ', 'ውስጥ', 'በማለፍ', 'ብቃታቸውን', 'የሚያስመሰክሩበት', 'መሆኑን', 'ሰምተናል፡፡', 'የሚገጥሟቸውን', 'የተለያዩ', 'ፈተናዎች', 'በትእግስትና', 'በጥበብ', 'ማለፍ፣', 'ከሌሎች', 'ጋር', 'ተስማምቶ', 'መዝለቅ፣', 'ችግሮችን', 'በብልጠት', 'መፍታት', 'ወዘተ', 'በየጊዜው', 'ከሚደረገው', 'ቅነሳ', 'ተርፈው', 'ለ91', 'ቀናት', 'ያህል', 'በውድድሩ', 'መቆየት', 'የቻሉ', 'ሁለት', 'ተወዳዳሪዎች', 'እያንዳንዳቸው', '200', 'ሺህ', 'ዶላር', 'እንደሚሸለሙም', 'ሲናገር', 'ነበር፡፡', 'በዘንድሮው', 'ውድድር', 'አገራችን', 'ዳኒ', 'እና', 'ሃኒ', 'የተባሉ', 'ሁለት', 'ወጣቶችን', 'ብታሰልፍም', 'ዳኒ', 'ቀደም', 'ብሎ', 'የቅነሳው', 'ሰለባ', 'ሲሆን', 'ሃኒም', 'በቅርቡ', 'ከውድድር', 'ውጭ', 'ሆናለች፡፡ይህቺን', 'የአገሪቱ', 'ብቸኛ', 'ተስፋ', 'ወደ', 'አሸናፊነት', 'ለማሸጋገር', 'የህዝብ', 'የድጋፍ', 'ድም', 'ወሳኝ', 'መሆኑን', 'የተገነዘበው', 'ወዳጄ', 'ነው', 'እንግዲህ', '835', 'የሚል', 'አገራዊ', 'ጥሪ', 'ያስተላለፈልኝ', 'ያኔ', 'ሃኒ', 'ከውድድሩ', 'ከመሰናበቷ', 'በፊት፡፡ወዳጄ', 'የአገሩን', 'ስም', 'በአሸናፊነት', 'የማስጠራት', 'ከፍተኛ', 'ጉጉት፣', 'አገሬ', 'እንዳትሸነፍ', 'የሚል', 'ከፍተኛ', 'ስጋት', 'እንዳደረበት', 'ይሰማኛል፡፡', 'ጉጉቱ', 'ሳይሆን', 'ስጋቱ', 'የወዳጄን', 'የዋህነት', '፡፡ሃኒም', 'ኢትዮጵያም', 'ይሸነፉ', 'ይሆን', 'በሚል', 'እንዲህ', 'ከንቱ', 'ስጋት', 'የሚያንገበግባቸውን', 'አገር', 'ወዳድ', 'ዜጐች', 'እኔ', 'የዋሆች', 'እላቸዋለሁ፡፡የዋሆች', 'ሆይ', 'አትስጉ', 'ስለ', 'ሃኒም', 'ስለ', 'ኢትዮጵያም', 'አትስጉ፡፡', 'ውድድሩ', 'ቢግ', 'ብራዘርስ', 'አፍሪካ', 'በአርቴፊሻል', 'ፈተናዎች', 'ውስጥ', 'አልፎ', 'ለሦስት', 'ወራት', 'የመቆየት', 'ውድድር', 'ነው፡፡', 'ደቡብ', 'አፍሪካ', 'ለስድስተኛ', 'ጊዜ']\n\nSize of valid tokens are:  80769388 Words(tokens)\n","output_type":"stream"}]},{"cell_type":"code","source":"start_time = time.time()\ntokens1 = nltk.word_tokenize(text)\nend_time = time.time()\nprint(f\"Time taken for tokenizetion:  {(end_time - start_time)/60}  minutes\")","metadata":{"execution":{"iopub.status.busy":"2023-11-21T22:17:07.658079Z","iopub.execute_input":"2023-11-21T22:17:07.658551Z","iopub.status.idle":"2023-11-21T22:29:23.787244Z","shell.execute_reply.started":"2023-11-21T22:17:07.658509Z","shell.execute_reply":"2023-11-21T22:29:23.785891Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Time taken for tokenizetion:  12.268705590565999  minutes\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Here, the entire file's content is read into one string, and then NLTK's word_tokenize function is used to tokenize the string into words. This method is straightforward and leverages the robust tokenization capabilities of the NLTK library. Now we have list of tokens in a single list variable `tokens1` but it is not `Efficient` so we use tokenization function tokenizer where built from scratch , because it tooks `12.22` minutes, almost 6 times customized one which is `2.24`  minutes.","metadata":{}},{"cell_type":"code","source":"print('Token size ',len(tokens))","metadata":{"execution":{"iopub.status.busy":"2023-11-21T22:29:23.788810Z","iopub.execute_input":"2023-11-21T22:29:23.789331Z","iopub.status.idle":"2023-11-21T22:29:23.796447Z","shell.execute_reply.started":"2023-11-21T22:29:23.789286Z","shell.execute_reply":"2023-11-21T22:29:23.795250Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Token size  80769388\n","output_type":"stream"}]},{"cell_type":"code","source":"print('\\n\\n Sample  from the tokens: first 500 words are :\\n')\nprint(tokens[:500])","metadata":{"execution":{"iopub.status.busy":"2023-11-21T22:29:23.797933Z","iopub.execute_input":"2023-11-21T22:29:23.798361Z","iopub.status.idle":"2023-11-21T22:29:23.810096Z","shell.execute_reply.started":"2023-11-21T22:29:23.798319Z","shell.execute_reply":"2023-11-21T22:29:23.808802Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"\n\n Sample  from the tokens: first 500 words are :\n\n['ምን', 'መሰላችሁ', 'አንባቢያን', 'ኢትዮጵያ', 'በተደጋጋሚ', 'ጥሪው', 'ደርሷት', 'ልትታደመው', 'ያልቻለችው', 'የአለም', 'የእግር', 'ኳስ', 'ዋ', 'ለ19ኛ', 'ጊዜ', 'በደቡብ', 'አፍሪካ', 'ሲጠጣ፣', 'በሩቅ', 'እያየች', 'አንጀቷ', 'ባረረ', 'ልክ', 'በአመቱ', 'በለስ', 'ቀናትና', 'ሌላ', 'ዋ', 'ልትታደም', 'ሁለት', 'ልጆቿን', 'ወደ', 'ደቡብ', 'አፍሪካ', 'ላከች፡፡6ኛው', 'ቢግ', 'ብራዘርስ', 'አፍሪካ', 'አብሮ', 'የመኖር', 'ውድድር', 'በደቡብ', 'አፍሪካ', 'ተካሂዷል፡፡', 'ከተለያዩ', '14', 'የአፍሪካ', 'አገራት', 'የተውጣጡ', '26', 'ያህል', 'ተሳታፊዎች', 'የተካፈሉበት', 'ይህ', 'ውድድር፣', 'ግለሰቦች', 'በፈታኝ', 'ሁኔታ', 'ውስጥ', 'በማለፍ', 'ብቃታቸውን', 'የሚያስመሰክሩበት', 'መሆኑን', 'ሰምተናል፡፡', 'የሚገጥሟቸውን', 'የተለያዩ', 'ፈተናዎች', 'በትእግስትና', 'በጥበብ', 'ማለፍ፣', 'ከሌሎች', 'ጋር', 'ተስማምቶ', 'መዝለቅ፣', 'ችግሮችን', 'በብልጠት', 'መፍታት', 'ወዘተ', 'በየጊዜው', 'ከሚደረገው', 'ቅነሳ', 'ተርፈው', 'ለ91', 'ቀናት', 'ያህል', 'በውድድሩ', 'መቆየት', 'የቻሉ', 'ሁለት', 'ተወዳዳሪዎች', 'እያንዳንዳቸው', '200', 'ሺህ', 'ዶላር', 'እንደሚሸለሙም', 'ሲናገር', 'ነበር፡፡', 'በዘንድሮው', 'ውድድር', 'አገራችን', 'ዳኒ', 'እና', 'ሃኒ', 'የተባሉ', 'ሁለት', 'ወጣቶችን', 'ብታሰልፍም', 'ዳኒ', 'ቀደም', 'ብሎ', 'የቅነሳው', 'ሰለባ', 'ሲሆን', 'ሃኒም', 'በቅርቡ', 'ከውድድር', 'ውጭ', 'ሆናለች፡፡ይህቺን', 'የአገሪቱ', 'ብቸኛ', 'ተስፋ', 'ወደ', 'አሸናፊነት', 'ለማሸጋገር', 'የህዝብ', 'የድጋፍ', 'ድም', 'ወሳኝ', 'መሆኑን', 'የተገነዘበው', 'ወዳጄ', 'ነው', 'እንግዲህ', '835', 'የሚል', 'አገራዊ', 'ጥሪ', 'ያስተላለፈልኝ', 'ያኔ', 'ሃኒ', 'ከውድድሩ', 'ከመሰናበቷ', 'በፊት፡፡ወዳጄ', 'የአገሩን', 'ስም', 'በአሸናፊነት', 'የማስጠራት', 'ከፍተኛ', 'ጉጉት፣', 'አገሬ', 'እንዳትሸነፍ', 'የሚል', 'ከፍተኛ', 'ስጋት', 'እንዳደረበት', 'ይሰማኛል፡፡', 'ጉጉቱ', 'ሳይሆን', 'ስጋቱ', 'የወዳጄን', 'የዋህነት', '፡፡ሃኒም', 'ኢትዮጵያም', 'ይሸነፉ', 'ይሆን', 'በሚል', 'እንዲህ', 'ከንቱ', 'ስጋት', 'የሚያንገበግባቸውን', 'አገር', 'ወዳድ', 'ዜጐች', 'እኔ', 'የዋሆች', 'እላቸዋለሁ፡፡የዋሆች', 'ሆይ', 'አትስጉ', 'ስለ', 'ሃኒም', 'ስለ', 'ኢትዮጵያም', 'አትስጉ፡፡', 'ውድድሩ', 'ቢግ', 'ብራዘርስ', 'አፍሪካ', 'በአርቴፊሻል', 'ፈተናዎች', 'ውስጥ', 'አልፎ', 'ለሦስት', 'ወራት', 'የመቆየት', 'ውድድር', 'ነው፡፡', 'ደቡብ', 'አፍሪካ', 'ለስድስተኛ', 'ጊዜ', 'ካዘጋጀችው', 'ቢግ', 'ብራዘርስ', 'የመረረ', 'ውድድር', 'ለሺህ', 'አመታት', 'በተከታታይ', 'ስታዘጋጅ', 'የኖረች፣', 'መላ', 'ህዝቧን', 'አሳትፋ', 'መላ', 'ህዝቧን', 'ስትሸልም', 'የኖረች', 'አገር', 'ናት', 'ኢትዮጵያ', 'የደቡብ', 'አፍሪካው', 'እንጂ', 'የኢትዮጵያው', 'ቢግ', 'ብራዘርስ', 'ሶስት', 'ወር', 'ተብሎ', 'ቀን', 'የሚቆረጥለት', 'ውስን', 'የፈተና', 'ጊዜ', 'የለውም፡፡', 'አንዲት', 'ኢትዮጵያ', 'በአንዲት', 'ሃኒ', 'ሳይሆን', 'በመላው', 'ህዝቧ', 'ነው', 'የምትወከለው', 'ለምን', 'ቢባል', 'ሶስት', 'ወር', 'ሳይሆን', 'ሶስት', 'ሺህ', 'ዘመን', 'በራሷ', 'ቢግ', 'ብራዘርስ', 'ተካፍላ', 'የምንጊዜም', 'አሸናፊ', 'ሆና', 'ዘልቃለችና፡፡ኢትዮጵያ', 'ለዘመናት', 'ባስተናገደችው', 'የራሷ', 'ቢግ', 'ብራዘርስ', 'አቻ', 'የለሽ', 'ፈተና', 'ውስጥ', 'መላው', 'ህዝቧን', 'እያሳተፈች', 'ድሉን', 'ከህዝቧ', 'እጅ', 'ባለማስነጠቅ', 'ሃትሪክ', 'የሰራች', 'በሺህ', 'አመታት', 'ስሌት', 'የምንጊዜም', 'ድል', 'ባለቤት', 'እኮ', 'ናት', 'የዋሆች', 'ስለ', 'ደቡብ', 'አፍሪካው', 'ቢግ', 'ብራዘር', 'ስለምን', 'ትጨነቃላችሁ', 'ሃኒ', 'እኮ', 'ለእግር', 'ኳስ', 'አይደለም', 'የሄደችው፡፡', 'እሱንማ', 'ብለነው', 'ብለነው', 'አልሆን', 'ብሎ', 'ቸግሮናል፡፡', 'ደረጃችን', 'ከሌሎች', 'በታች', 'ሆኖ', 'ቀርቶብን', 'በሩቅ', 'እያየነው', 'ተብሰልስለናል፡፡', 'አሁን', 'ሃኒን', 'ወደ', 'ደቡብ', 'አፍሪካ', 'የላክናት', 'ከአቅሟ', 'በላይ', 'ሳይሆን', 'በታች', 'ወርዳ', 'ወደምትጫወትበት', 'አብሮ', 'የመኖር', 'ቀላል', 'ፉክክር', 'ነው፡፡', 'ሰቆቃንና', 'ፈተናን', 'ተጋፍጦ', 'በመኖር', 'የአለም', 'ሻምፒዮናነቱን', 'አለም', 'በአንድ', 'ድም', 'ያፀደቀለት', 'አሸናፊ', 'ህዝብ', 'ወኪል', 'የሆነችው', 'ሃኒ፣', 'ያለ', 'ዲቪዚዮኗ', 'ስንትና', 'ስንት', 'ቁልቁል', 'ወርዳ', 'እኮ', 'ነው', 'የተወዳደረችው፡፡ሃኒ', 'ከቢግ', 'ብራዘር', 'ውድድር', 'ውጭ', 'መሆኗን', 'ሰሞኑን', 'ሰማሁ፡፡', 'ሰማሁና', 'ሳቅኩ፡፡', 'ለምን', 'ሳቅኩ', 'የአገሬ', 'መሸነፍ', 'የማያንገበግበኝ', 'ሰው', 'ሆኜ', 'ነውን', 'አይመስለኝም', 'ሃኒ', 'ከውድድሩ', 'ውጭ', 'የሆነችው', 'የቢግ', 'ብራዘርስ', 'አብሮ', 'የመኖር', 'ውድድር', 'አሸንፏት', 'ወይም', 'አቅቷት', 'አይመስለኝም', 'እሷ', 'ከመስፈርቱ', 'በላይ', 'ሆና', 'እንጂ', 'እንደ', 'ለት', 'ኢትዮጵያ', 'ከ6ኛው', 'የቢግ', 'ብራዘርስ', 'አፍሪካ', 'ውድድር', 'የተሰናበተችው', 'በቀላል', 'ሚዛን', 'ውድድር', 'የከባድ', 'ሚዛን', 'ተወዳዳሪ', 'በማሰለፏ', 'ነው', 'ባይ', 'ነኝ፡፡', 'ልክ', 'በእግር', 'ኳስ', 'ውድድር', 'ላይ', 'እንደሚከሰተው', 'ለምሳሌ', 'ከ16', 'አመት', 'በታች', 'የሆኑ', 'ተጫዋቾች', 'በሚሳተፉበት', 'የታዳጊ', 'ወጣቶች', 'ሻምፒዮና', 'ላይ', 'የ25', 'አመት', 'እድሜ', 'ያለው', 'ተጫዋች', 'በማሰለፉ', 'ከውድድር', 'ውጭ', 'እንደሚሆን', 'ክለብ፡፡ይመስለኛል', 'ሃኒ', 'ከውድድሩ', 'የተባረረችው', 'የቢግ', 'ብራዘርስ', 'ፈተና', 'ስላቃታት', 'አይደለም፡፡', 'ምናልባትም', 'እሷ', 'ለፈተናው', 'ከብዳው', 'ቢሆን', 'እንጂ፡፡', 'ሃኒ', 'ከቢግ', 'ብራዘር', 'ግቢ', 'ተባረረች', 'የሚሉ፣', 'እነሱ', 'አንዷን', 'ሃኒ', 'ብቻ', 'የሚያዩ', 'የዋሆች', 'ናቸው፡፡', 'ሃኒ', 'እዚያው', 'ደቡብ', 'አፍሪካ', 'ናት', 'ደቡብ', 'አፍሪካ', 'ኡጋንዳ', 'ኬኒያ', 'ሊቢያ', 'ሻሸመኔ', 'ቤሩት', 'አሜሪካ', 'እዚህም', 'እዚያም', 'ተበትና', 'ከቢግ', 'ብራዘር', 'የመረረ', 'ሰቆቃ', 'ውስጥ', 'እየተንገላታች', 'ችግር', 'ቻይነቷን', 'እያስመሰከረች', 'ያለች', 'ብዙ', 'ኢትዮጵያዊ', 'ሴት', 'ናት', 'ሃኒ፡፡ቢግ', 'ብራዘር', 'በሚሉት', 'የፌክ', 'ፈተና', 'እና']\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Save the tokens for later use and change the address to none temporary directory or download and manualy upload to fixed directory\nThe Kaggle working directory is temporary, and it deletes files upon restarting. To ensure the preservation of tokens for later use, one option is to save them and change the directory to a non-temporary location. Alternatively, the tokens can be downloaded and manually uploaded to a fixed directory. In my case, I will manually upload the saved tokens to '/kaggle/input/saved-amharic-tokens'.","metadata":{}},{"cell_type":"code","source":"# The Kaggle working directory is temporary, and it deletes files upon restarting. \n# To ensure the preservation of tokens for later use, one option is to save them and change the directory to a non-temporary location. \nwith open('/kaggle/working/tokens.pkl', 'wb') as file:\n    pickle.dump(tokens, file)","metadata":{"execution":{"iopub.status.busy":"2023-11-21T22:29:23.814100Z","iopub.execute_input":"2023-11-21T22:29:23.814437Z","iopub.status.idle":"2023-11-21T22:29:23.821254Z","shell.execute_reply.started":"2023-11-21T22:29:23.814408Z","shell.execute_reply":"2023-11-21T22:29:23.820442Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"### Load saved tokens we have to load the token from different directory ","metadata":{}},{"cell_type":"code","source":"# # Load saved tokens we have to load the token from different directory \nwith open('/kaggle/input/saved-tokens/tokens.pkl', 'rb') as file:\n    tokens = pickle.load(file)","metadata":{"execution":{"iopub.status.busy":"2023-11-21T22:29:23.822300Z","iopub.execute_input":"2023-11-21T22:29:23.823196Z","iopub.status.idle":"2023-11-21T22:29:23.832259Z","shell.execute_reply.started":"2023-11-21T22:29:23.823165Z","shell.execute_reply":"2023-11-21T22:29:23.831379Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### Clean the corpus from amharic stopwords, English symbols, Numbers, amharic punctuations\nAfter tokenization, the next step is to clean the corpus by removing stopwords, symbols, and punctuation from both the Amharic and English languages.","metadata":{}},{"cell_type":"code","source":"# A sample list of custom stopwords in Amharic\nimport string\n\n# Define a list of custom stopwords in Amharic\namharic_stopwords = [] \n\n# Define Amharic punctuations\namharic_punctuations = ['፡', '።', '፣', '፤', '፥', '፦', '፧', '፨']\n\n# English symbols that we want to remove (this includes letters, digits, and punctuation)\nenglish_symbols = list(string.ascii_letters + string.digits + string.punctuation)\n\n# Combined list of characters to remove\nelemnts_to_remove = amharic_stopwords + amharic_punctuations + english_symbols\nstart_time = time.time()\ncleaned_tokens = [token for token in tokens if not any(ch in elemnts_to_remove for ch in token) and not token.isdigit()]\n\nprint('\\nTokens removed \\n',elemnts_to_remove)\n\nend_time = time.time()\nprint(f\"\\n\\nTime taken for cleaning  :  {(end_time - start_time)/60}  minutes \\n\")","metadata":{"execution":{"iopub.status.busy":"2023-11-21T22:29:23.844304Z","iopub.execute_input":"2023-11-21T22:29:23.844653Z","iopub.status.idle":"2023-11-21T22:39:21.669866Z","shell.execute_reply.started":"2023-11-21T22:29:23.844624Z","shell.execute_reply":"2023-11-21T22:39:21.668703Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"\nTokens removed \n ['፡', '።', '፣', '፤', '፥', '፦', '፧', '፨', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n\n\nTime taken for cleaning  :  9.963485936323801  minutes \n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"First we define the unwanted elements, including Amharic stopwords, Amharic punctuation, and English symbols (comprising letters, digits, and punctuation). The then iterates through the list of tokens and excludes any token that contains these elements or is composed solely of digits. The resulting cleaned_tokens list contains the text data cleansed of irrelevant characters and symbols, ready for further text analysis or processing tasks.\n\nIt then cleans the tokenized data by removing any tokens that contain these defined characters or are purely numeric. The result is a list of cleaned tokens free from unwanted characters. ","metadata":{}},{"cell_type":"code","source":"print('\\nToken size after cleaned: ', len(cleaned_tokens))\nprint('\\nThe first 50 words from the tokens \\n',cleaned_tokens[:150])\n\nprint('\\nToken size befor cleaned: ',len(tokens))\nprint('\\nThe first 50 words from the original tokens \\n',tokens[:150])\n\n\n\nprint('\\nLength difference/Number of removed tokens: ',len(tokens)-len(cleaned_tokens) )","metadata":{"execution":{"iopub.status.busy":"2023-11-21T22:39:21.671533Z","iopub.execute_input":"2023-11-21T22:39:21.672488Z","iopub.status.idle":"2023-11-21T22:39:21.681319Z","shell.execute_reply.started":"2023-11-21T22:39:21.672417Z","shell.execute_reply":"2023-11-21T22:39:21.679820Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"\nToken size after cleaned:  71813265\n\nThe first 50 words from the tokens \n ['ምን', 'መሰላችሁ', 'አንባቢያን', 'ኢትዮጵያ', 'በተደጋጋሚ', 'ጥሪው', 'ደርሷት', 'ልትታደመው', 'ያልቻለችው', 'የአለም', 'የእግር', 'ኳስ', 'ዋ', 'ጊዜ', 'በደቡብ', 'አፍሪካ', 'በሩቅ', 'እያየች', 'አንጀቷ', 'ባረረ', 'ልክ', 'በአመቱ', 'በለስ', 'ቀናትና', 'ሌላ', 'ዋ', 'ልትታደም', 'ሁለት', 'ልጆቿን', 'ወደ', 'ደቡብ', 'አፍሪካ', 'ቢግ', 'ብራዘርስ', 'አፍሪካ', 'አብሮ', 'የመኖር', 'ውድድር', 'በደቡብ', 'አፍሪካ', 'ከተለያዩ', 'የአፍሪካ', 'አገራት', 'የተውጣጡ', 'ያህል', 'ተሳታፊዎች', 'የተካፈሉበት', 'ይህ', 'ግለሰቦች', 'በፈታኝ', 'ሁኔታ', 'ውስጥ', 'በማለፍ', 'ብቃታቸውን', 'የሚያስመሰክሩበት', 'መሆኑን', 'የሚገጥሟቸውን', 'የተለያዩ', 'ፈተናዎች', 'በትእግስትና', 'በጥበብ', 'ከሌሎች', 'ጋር', 'ተስማምቶ', 'ችግሮችን', 'በብልጠት', 'መፍታት', 'ወዘተ', 'በየጊዜው', 'ከሚደረገው', 'ቅነሳ', 'ተርፈው', 'ቀናት', 'ያህል', 'በውድድሩ', 'መቆየት', 'የቻሉ', 'ሁለት', 'ተወዳዳሪዎች', 'እያንዳንዳቸው', 'ሺህ', 'ዶላር', 'እንደሚሸለሙም', 'ሲናገር', 'በዘንድሮው', 'ውድድር', 'አገራችን', 'ዳኒ', 'እና', 'ሃኒ', 'የተባሉ', 'ሁለት', 'ወጣቶችን', 'ብታሰልፍም', 'ዳኒ', 'ቀደም', 'ብሎ', 'የቅነሳው', 'ሰለባ', 'ሲሆን', 'ሃኒም', 'በቅርቡ', 'ከውድድር', 'ውጭ', 'የአገሪቱ', 'ብቸኛ', 'ተስፋ', 'ወደ', 'አሸናፊነት', 'ለማሸጋገር', 'የህዝብ', 'የድጋፍ', 'ድም', 'ወሳኝ', 'መሆኑን', 'የተገነዘበው', 'ወዳጄ', 'ነው', 'እንግዲህ', 'የሚል', 'አገራዊ', 'ጥሪ', 'ያስተላለፈልኝ', 'ያኔ', 'ሃኒ', 'ከውድድሩ', 'ከመሰናበቷ', 'የአገሩን', 'ስም', 'በአሸናፊነት', 'የማስጠራት', 'ከፍተኛ', 'አገሬ', 'እንዳትሸነፍ', 'የሚል', 'ከፍተኛ', 'ስጋት', 'እንዳደረበት', 'ጉጉቱ', 'ሳይሆን', 'ስጋቱ', 'የወዳጄን', 'የዋህነት', 'ኢትዮጵያም', 'ይሸነፉ', 'ይሆን', 'በሚል', 'እንዲህ', 'ከንቱ', 'ስጋት']\n\nToken size befor cleaned:  80769388\n\nThe first 50 words from the original tokens \n ['ምን', 'መሰላችሁ', 'አንባቢያን', 'ኢትዮጵያ', 'በተደጋጋሚ', 'ጥሪው', 'ደርሷት', 'ልትታደመው', 'ያልቻለችው', 'የአለም', 'የእግር', 'ኳስ', 'ዋ', 'ለ19ኛ', 'ጊዜ', 'በደቡብ', 'አፍሪካ', 'ሲጠጣ፣', 'በሩቅ', 'እያየች', 'አንጀቷ', 'ባረረ', 'ልክ', 'በአመቱ', 'በለስ', 'ቀናትና', 'ሌላ', 'ዋ', 'ልትታደም', 'ሁለት', 'ልጆቿን', 'ወደ', 'ደቡብ', 'አፍሪካ', 'ላከች፡፡6ኛው', 'ቢግ', 'ብራዘርስ', 'አፍሪካ', 'አብሮ', 'የመኖር', 'ውድድር', 'በደቡብ', 'አፍሪካ', 'ተካሂዷል፡፡', 'ከተለያዩ', '14', 'የአፍሪካ', 'አገራት', 'የተውጣጡ', '26', 'ያህል', 'ተሳታፊዎች', 'የተካፈሉበት', 'ይህ', 'ውድድር፣', 'ግለሰቦች', 'በፈታኝ', 'ሁኔታ', 'ውስጥ', 'በማለፍ', 'ብቃታቸውን', 'የሚያስመሰክሩበት', 'መሆኑን', 'ሰምተናል፡፡', 'የሚገጥሟቸውን', 'የተለያዩ', 'ፈተናዎች', 'በትእግስትና', 'በጥበብ', 'ማለፍ፣', 'ከሌሎች', 'ጋር', 'ተስማምቶ', 'መዝለቅ፣', 'ችግሮችን', 'በብልጠት', 'መፍታት', 'ወዘተ', 'በየጊዜው', 'ከሚደረገው', 'ቅነሳ', 'ተርፈው', 'ለ91', 'ቀናት', 'ያህል', 'በውድድሩ', 'መቆየት', 'የቻሉ', 'ሁለት', 'ተወዳዳሪዎች', 'እያንዳንዳቸው', '200', 'ሺህ', 'ዶላር', 'እንደሚሸለሙም', 'ሲናገር', 'ነበር፡፡', 'በዘንድሮው', 'ውድድር', 'አገራችን', 'ዳኒ', 'እና', 'ሃኒ', 'የተባሉ', 'ሁለት', 'ወጣቶችን', 'ብታሰልፍም', 'ዳኒ', 'ቀደም', 'ብሎ', 'የቅነሳው', 'ሰለባ', 'ሲሆን', 'ሃኒም', 'በቅርቡ', 'ከውድድር', 'ውጭ', 'ሆናለች፡፡ይህቺን', 'የአገሪቱ', 'ብቸኛ', 'ተስፋ', 'ወደ', 'አሸናፊነት', 'ለማሸጋገር', 'የህዝብ', 'የድጋፍ', 'ድም', 'ወሳኝ', 'መሆኑን', 'የተገነዘበው', 'ወዳጄ', 'ነው', 'እንግዲህ', '835', 'የሚል', 'አገራዊ', 'ጥሪ', 'ያስተላለፈልኝ', 'ያኔ', 'ሃኒ', 'ከውድድሩ', 'ከመሰናበቷ', 'በፊት፡፡ወዳጄ', 'የአገሩን', 'ስም', 'በአሸናፊነት', 'የማስጠራት', 'ከፍተኛ', 'ጉጉት፣', 'አገሬ']\n\nLength difference/Number of removed tokens:  8956123\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Save the cleaned tokens for later use","metadata":{}},{"cell_type":"code","source":"# Save the tokens for later use\nwith open('/kaggle/working/tokens_cleaned.pkl', 'wb') as file:\n    pickle.dump(cleaned_tokens, file)","metadata":{"execution":{"iopub.status.busy":"2023-11-21T22:39:21.682780Z","iopub.execute_input":"2023-11-21T22:39:21.683119Z","iopub.status.idle":"2023-11-21T22:39:21.692931Z","shell.execute_reply.started":"2023-11-21T22:39:21.683090Z","shell.execute_reply":"2023-11-21T22:39:21.691795Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# # Load saved clean_Tokens, we have to load the token from different directory \n\nwith open('/kaggle/input/saved-tokens/tokens_cleaned(1).pkl', 'rb') as file:\n    tokens = pickle.load(file)\n\n# with open('/kaggle/working/tokens_cleaned.pkl', 'rb') as file:\n#     tokens = pickle.load(file)","metadata":{"execution":{"iopub.status.busy":"2023-11-21T22:39:21.694505Z","iopub.execute_input":"2023-11-21T22:39:21.694935Z","iopub.status.idle":"2023-11-21T22:39:28.766761Z","shell.execute_reply.started":"2023-11-21T22:39:21.694894Z","shell.execute_reply":"2023-11-21T22:39:28.765639Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"<a name=\"1[](http://)\"></a>\n## N-gram language mode","metadata":{}},{"cell_type":"markdown","source":"<a name=\"1.1\"></a>\n###  1.1 Create n-grams for n=1, 2, 3, 4.","metadata":{}},{"cell_type":"markdown","source":"<a name=\"99\"></a>\n- [ Top](#99)","metadata":{}},{"cell_type":"code","source":"# Extract n-grams n-gram generators and change to list\nstart_time = time.time()\nunigrams = ngrams(tokens, 1)\nbigrams = ngrams(tokens, 2)\ntrigrams = ngrams(tokens, 3)\nfourgrams = ngrams(tokens, 4)\nend_time = time.time()\nprint(f\"\\n\\nTime taken for  n-gram generatoin :  {(end_time - start_time)/60}  minutes \\n\")\n\n\n# we can change it to list but it is not memory efficient\nunigramslist = list(ngrams(tokens, 1)) \nbigramslist = list(ngrams(tokens, 2))\ntrigramslist = list(ngrams(tokens, 3))\nfourgramslist = list(ngrams(tokens, 4))","metadata":{"execution":{"iopub.status.busy":"2023-11-21T22:39:28.787899Z","iopub.execute_input":"2023-11-21T22:39:28.788601Z","iopub.status.idle":"2023-11-21T22:39:54.528787Z","shell.execute_reply.started":"2023-11-21T22:39:28.788566Z","shell.execute_reply":"2023-11-21T22:39:54.527557Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"\n\nTime taken for  n-gram generatoin :  2.5510787963867186e-06  minutes \n\n","output_type":"stream"}]},{"cell_type":"code","source":"print(fourgramslist[:10])","metadata":{"execution":{"iopub.status.busy":"2023-11-21T22:39:54.530202Z","iopub.execute_input":"2023-11-21T22:39:54.530558Z","iopub.status.idle":"2023-11-21T22:39:54.536193Z","shell.execute_reply.started":"2023-11-21T22:39:54.530528Z","shell.execute_reply":"2023-11-21T22:39:54.535060Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"[('ምን', 'መሰላችሁ', 'አንባቢያን', 'ኢትዮጵያ'), ('መሰላችሁ', 'አንባቢያን', 'ኢትዮጵያ', 'በተደጋጋሚ'), ('አንባቢያን', 'ኢትዮጵያ', 'በተደጋጋሚ', 'ጥሪው'), ('ኢትዮጵያ', 'በተደጋጋሚ', 'ጥሪው', 'ደርሷት'), ('በተደጋጋሚ', 'ጥሪው', 'ደርሷት', 'ልትታደመው'), ('ጥሪው', 'ደርሷት', 'ልትታደመው', 'ያልቻለችው'), ('ደርሷት', 'ልትታደመው', 'ያልቻለችው', 'የአለም'), ('ልትታደመው', 'ያልቻለችው', 'የአለም', 'የእግር'), ('ያልቻለችው', 'የአለም', 'የእግር', 'ኳስ'), ('የአለም', 'የእግር', 'ኳስ', 'ዋ')]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Print samples first 50 n-grams for all 4 n-grams","metadata":{}},{"cell_type":"code","source":"print('\\n\\nFirst 50 Unigrams\\n')\nprint(unigramslist[:50])\nprint('\\n')\nprint('\\nFirst 50 Bigrams\\n')\nprint(bigramslist[:50])\nprint('\\n')\nprint('\\nFirst 50 Trigrams\\n')\nprint(trigramslist[:50])\nprint('\\n')\nprint('\\nFirst 50 fourgrams\\n')\nprint(fourgramslist[:55])","metadata":{"execution":{"iopub.status.busy":"2023-11-21T22:39:54.537574Z","iopub.execute_input":"2023-11-21T22:39:54.537898Z","iopub.status.idle":"2023-11-21T22:39:54.557578Z","shell.execute_reply.started":"2023-11-21T22:39:54.537867Z","shell.execute_reply":"2023-11-21T22:39:54.556479Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"\n\nFirst 50 Unigrams\n\n[('ምን',), ('መሰላችሁ',), ('አንባቢያን',), ('ኢትዮጵያ',), ('በተደጋጋሚ',), ('ጥሪው',), ('ደርሷት',), ('ልትታደመው',), ('ያልቻለችው',), ('የአለም',), ('የእግር',), ('ኳስ',), ('ዋ',), ('ጊዜ',), ('በደቡብ',), ('አፍሪካ',), ('በሩቅ',), ('እያየች',), ('አንጀቷ',), ('ባረረ',), ('ልክ',), ('በአመቱ',), ('በለስ',), ('ቀናትና',), ('ሌላ',), ('ዋ',), ('ልትታደም',), ('ሁለት',), ('ልጆቿን',), ('ወደ',), ('ደቡብ',), ('አፍሪካ',), ('ቢግ',), ('ብራዘርስ',), ('አፍሪካ',), ('አብሮ',), ('የመኖር',), ('ውድድር',), ('በደቡብ',), ('አፍሪካ',), ('ከተለያዩ',), ('የአፍሪካ',), ('አገራት',), ('የተውጣጡ',), ('ያህል',), ('ተሳታፊዎች',), ('የተካፈሉበት',), ('ይህ',), ('ግለሰቦች',), ('በፈታኝ',)]\n\n\n\nFirst 50 Bigrams\n\n[('ምን', 'መሰላችሁ'), ('መሰላችሁ', 'አንባቢያን'), ('አንባቢያን', 'ኢትዮጵያ'), ('ኢትዮጵያ', 'በተደጋጋሚ'), ('በተደጋጋሚ', 'ጥሪው'), ('ጥሪው', 'ደርሷት'), ('ደርሷት', 'ልትታደመው'), ('ልትታደመው', 'ያልቻለችው'), ('ያልቻለችው', 'የአለም'), ('የአለም', 'የእግር'), ('የእግር', 'ኳስ'), ('ኳስ', 'ዋ'), ('ዋ', 'ጊዜ'), ('ጊዜ', 'በደቡብ'), ('በደቡብ', 'አፍሪካ'), ('አፍሪካ', 'በሩቅ'), ('በሩቅ', 'እያየች'), ('እያየች', 'አንጀቷ'), ('አንጀቷ', 'ባረረ'), ('ባረረ', 'ልክ'), ('ልክ', 'በአመቱ'), ('በአመቱ', 'በለስ'), ('በለስ', 'ቀናትና'), ('ቀናትና', 'ሌላ'), ('ሌላ', 'ዋ'), ('ዋ', 'ልትታደም'), ('ልትታደም', 'ሁለት'), ('ሁለት', 'ልጆቿን'), ('ልጆቿን', 'ወደ'), ('ወደ', 'ደቡብ'), ('ደቡብ', 'አፍሪካ'), ('አፍሪካ', 'ቢግ'), ('ቢግ', 'ብራዘርስ'), ('ብራዘርስ', 'አፍሪካ'), ('አፍሪካ', 'አብሮ'), ('አብሮ', 'የመኖር'), ('የመኖር', 'ውድድር'), ('ውድድር', 'በደቡብ'), ('በደቡብ', 'አፍሪካ'), ('አፍሪካ', 'ከተለያዩ'), ('ከተለያዩ', 'የአፍሪካ'), ('የአፍሪካ', 'አገራት'), ('አገራት', 'የተውጣጡ'), ('የተውጣጡ', 'ያህል'), ('ያህል', 'ተሳታፊዎች'), ('ተሳታፊዎች', 'የተካፈሉበት'), ('የተካፈሉበት', 'ይህ'), ('ይህ', 'ግለሰቦች'), ('ግለሰቦች', 'በፈታኝ'), ('በፈታኝ', 'ሁኔታ')]\n\n\n\nFirst 50 Trigrams\n\n[('ምን', 'መሰላችሁ', 'አንባቢያን'), ('መሰላችሁ', 'አንባቢያን', 'ኢትዮጵያ'), ('አንባቢያን', 'ኢትዮጵያ', 'በተደጋጋሚ'), ('ኢትዮጵያ', 'በተደጋጋሚ', 'ጥሪው'), ('በተደጋጋሚ', 'ጥሪው', 'ደርሷት'), ('ጥሪው', 'ደርሷት', 'ልትታደመው'), ('ደርሷት', 'ልትታደመው', 'ያልቻለችው'), ('ልትታደመው', 'ያልቻለችው', 'የአለም'), ('ያልቻለችው', 'የአለም', 'የእግር'), ('የአለም', 'የእግር', 'ኳስ'), ('የእግር', 'ኳስ', 'ዋ'), ('ኳስ', 'ዋ', 'ጊዜ'), ('ዋ', 'ጊዜ', 'በደቡብ'), ('ጊዜ', 'በደቡብ', 'አፍሪካ'), ('በደቡብ', 'አፍሪካ', 'በሩቅ'), ('አፍሪካ', 'በሩቅ', 'እያየች'), ('በሩቅ', 'እያየች', 'አንጀቷ'), ('እያየች', 'አንጀቷ', 'ባረረ'), ('አንጀቷ', 'ባረረ', 'ልክ'), ('ባረረ', 'ልክ', 'በአመቱ'), ('ልክ', 'በአመቱ', 'በለስ'), ('በአመቱ', 'በለስ', 'ቀናትና'), ('በለስ', 'ቀናትና', 'ሌላ'), ('ቀናትና', 'ሌላ', 'ዋ'), ('ሌላ', 'ዋ', 'ልትታደም'), ('ዋ', 'ልትታደም', 'ሁለት'), ('ልትታደም', 'ሁለት', 'ልጆቿን'), ('ሁለት', 'ልጆቿን', 'ወደ'), ('ልጆቿን', 'ወደ', 'ደቡብ'), ('ወደ', 'ደቡብ', 'አፍሪካ'), ('ደቡብ', 'አፍሪካ', 'ቢግ'), ('አፍሪካ', 'ቢግ', 'ብራዘርስ'), ('ቢግ', 'ብራዘርስ', 'አፍሪካ'), ('ብራዘርስ', 'አፍሪካ', 'አብሮ'), ('አፍሪካ', 'አብሮ', 'የመኖር'), ('አብሮ', 'የመኖር', 'ውድድር'), ('የመኖር', 'ውድድር', 'በደቡብ'), ('ውድድር', 'በደቡብ', 'አፍሪካ'), ('በደቡብ', 'አፍሪካ', 'ከተለያዩ'), ('አፍሪካ', 'ከተለያዩ', 'የአፍሪካ'), ('ከተለያዩ', 'የአፍሪካ', 'አገራት'), ('የአፍሪካ', 'አገራት', 'የተውጣጡ'), ('አገራት', 'የተውጣጡ', 'ያህል'), ('የተውጣጡ', 'ያህል', 'ተሳታፊዎች'), ('ያህል', 'ተሳታፊዎች', 'የተካፈሉበት'), ('ተሳታፊዎች', 'የተካፈሉበት', 'ይህ'), ('የተካፈሉበት', 'ይህ', 'ግለሰቦች'), ('ይህ', 'ግለሰቦች', 'በፈታኝ'), ('ግለሰቦች', 'በፈታኝ', 'ሁኔታ'), ('በፈታኝ', 'ሁኔታ', 'ውስጥ')]\n\n\n\nFirst 50 fourgrams\n\n[('ምን', 'መሰላችሁ', 'አንባቢያን', 'ኢትዮጵያ'), ('መሰላችሁ', 'አንባቢያን', 'ኢትዮጵያ', 'በተደጋጋሚ'), ('አንባቢያን', 'ኢትዮጵያ', 'በተደጋጋሚ', 'ጥሪው'), ('ኢትዮጵያ', 'በተደጋጋሚ', 'ጥሪው', 'ደርሷት'), ('በተደጋጋሚ', 'ጥሪው', 'ደርሷት', 'ልትታደመው'), ('ጥሪው', 'ደርሷት', 'ልትታደመው', 'ያልቻለችው'), ('ደርሷት', 'ልትታደመው', 'ያልቻለችው', 'የአለም'), ('ልትታደመው', 'ያልቻለችው', 'የአለም', 'የእግር'), ('ያልቻለችው', 'የአለም', 'የእግር', 'ኳስ'), ('የአለም', 'የእግር', 'ኳስ', 'ዋ'), ('የእግር', 'ኳስ', 'ዋ', 'ጊዜ'), ('ኳስ', 'ዋ', 'ጊዜ', 'በደቡብ'), ('ዋ', 'ጊዜ', 'በደቡብ', 'አፍሪካ'), ('ጊዜ', 'በደቡብ', 'አፍሪካ', 'በሩቅ'), ('በደቡብ', 'አፍሪካ', 'በሩቅ', 'እያየች'), ('አፍሪካ', 'በሩቅ', 'እያየች', 'አንጀቷ'), ('በሩቅ', 'እያየች', 'አንጀቷ', 'ባረረ'), ('እያየች', 'አንጀቷ', 'ባረረ', 'ልክ'), ('አንጀቷ', 'ባረረ', 'ልክ', 'በአመቱ'), ('ባረረ', 'ልክ', 'በአመቱ', 'በለስ'), ('ልክ', 'በአመቱ', 'በለስ', 'ቀናትና'), ('በአመቱ', 'በለስ', 'ቀናትና', 'ሌላ'), ('በለስ', 'ቀናትና', 'ሌላ', 'ዋ'), ('ቀናትና', 'ሌላ', 'ዋ', 'ልትታደም'), ('ሌላ', 'ዋ', 'ልትታደም', 'ሁለት'), ('ዋ', 'ልትታደም', 'ሁለት', 'ልጆቿን'), ('ልትታደም', 'ሁለት', 'ልጆቿን', 'ወደ'), ('ሁለት', 'ልጆቿን', 'ወደ', 'ደቡብ'), ('ልጆቿን', 'ወደ', 'ደቡብ', 'አፍሪካ'), ('ወደ', 'ደቡብ', 'አፍሪካ', 'ቢግ'), ('ደቡብ', 'አፍሪካ', 'ቢግ', 'ብራዘርስ'), ('አፍሪካ', 'ቢግ', 'ብራዘርስ', 'አፍሪካ'), ('ቢግ', 'ብራዘርስ', 'አፍሪካ', 'አብሮ'), ('ብራዘርስ', 'አፍሪካ', 'አብሮ', 'የመኖር'), ('አፍሪካ', 'አብሮ', 'የመኖር', 'ውድድር'), ('አብሮ', 'የመኖር', 'ውድድር', 'በደቡብ'), ('የመኖር', 'ውድድር', 'በደቡብ', 'አፍሪካ'), ('ውድድር', 'በደቡብ', 'አፍሪካ', 'ከተለያዩ'), ('በደቡብ', 'አፍሪካ', 'ከተለያዩ', 'የአፍሪካ'), ('አፍሪካ', 'ከተለያዩ', 'የአፍሪካ', 'አገራት'), ('ከተለያዩ', 'የአፍሪካ', 'አገራት', 'የተውጣጡ'), ('የአፍሪካ', 'አገራት', 'የተውጣጡ', 'ያህል'), ('አገራት', 'የተውጣጡ', 'ያህል', 'ተሳታፊዎች'), ('የተውጣጡ', 'ያህል', 'ተሳታፊዎች', 'የተካፈሉበት'), ('ያህል', 'ተሳታፊዎች', 'የተካፈሉበት', 'ይህ'), ('ተሳታፊዎች', 'የተካፈሉበት', 'ይህ', 'ግለሰቦች'), ('የተካፈሉበት', 'ይህ', 'ግለሰቦች', 'በፈታኝ'), ('ይህ', 'ግለሰቦች', 'በፈታኝ', 'ሁኔታ'), ('ግለሰቦች', 'በፈታኝ', 'ሁኔታ', 'ውስጥ'), ('በፈታኝ', 'ሁኔታ', 'ውስጥ', 'በማለፍ'), ('ሁኔታ', 'ውስጥ', 'በማለፍ', 'ብቃታቸውን'), ('ውስጥ', 'በማለፍ', 'ብቃታቸውን', 'የሚያስመሰክሩበት'), ('በማለፍ', 'ብቃታቸውን', 'የሚያስመሰክሩበት', 'መሆኑን'), ('ብቃታቸውን', 'የሚያስመሰክሩበት', 'መሆኑን', 'የሚገጥሟቸውን'), ('የሚያስመሰክሩበት', 'መሆኑን', 'የሚገጥሟቸውን', 'የተለያዩ')]\n","output_type":"stream"}]},{"cell_type":"code","source":"# sample prints\n# print samples for all 4 n-grams\nprint('\\nUnigrams')\nprint(next(unigrams))\nprint(next(unigrams))\nprint(next(unigrams))\nprint(next(unigrams))\n\nprint('\\nBigrams')\nprint(next(bigrams))\nprint(next(bigrams))\nprint(next(bigrams))\nprint(next(bigrams))\n\nprint('\\nTrigrams')\nprint(next(trigrams))\nprint(next(trigrams))\nprint(next(trigrams))\nprint(next(trigrams))\nprint('\\nFourgrams')\n\nprint(next(fourgrams))\nprint(next(fourgrams))\nprint(next(fourgrams))\nprint(next(fourgrams))","metadata":{"execution":{"iopub.status.busy":"2023-11-21T22:39:54.558861Z","iopub.execute_input":"2023-11-21T22:39:54.559210Z","iopub.status.idle":"2023-11-21T22:39:54.573841Z","shell.execute_reply.started":"2023-11-21T22:39:54.559179Z","shell.execute_reply":"2023-11-21T22:39:54.572832Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"\nUnigrams\n('ምን',)\n('መሰላችሁ',)\n('አንባቢያን',)\n('ኢትዮጵያ',)\n\nBigrams\n('ምን', 'መሰላችሁ')\n('መሰላችሁ', 'አንባቢያን')\n('አንባቢያን', 'ኢትዮጵያ')\n('ኢትዮጵያ', 'በተደጋጋሚ')\n\nTrigrams\n('ምን', 'መሰላችሁ', 'አንባቢያን')\n('መሰላችሁ', 'አንባቢያን', 'ኢትዮጵያ')\n('አንባቢያን', 'ኢትዮጵያ', 'በተደጋጋሚ')\n('ኢትዮጵያ', 'በተደጋጋሚ', 'ጥሪው')\n\nFourgrams\n('ምን', 'መሰላችሁ', 'አንባቢያን', 'ኢትዮጵያ')\n('መሰላችሁ', 'አንባቢያን', 'ኢትዮጵያ', 'በተደጋጋሚ')\n('አንባቢያን', 'ኢትዮጵያ', 'በተደጋጋሚ', 'ጥሪው')\n('ኢትዮጵያ', 'በተደጋጋሚ', 'ጥሪው', 'ደርሷት')\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a name=\"1.2\"></a>\n###  1.2. Calculate probabilities of n-grams and find the top 10 most likely n-grams for all n.","metadata":{}},{"cell_type":"markdown","source":"<a name=\"99\"></a>\n- [ Top](#99)","metadata":{}},{"cell_type":"markdown","source":"### 1.2.1. probabilities of 2-grams and top 10 most likely 2-grams","metadata":{}},{"cell_type":"markdown","source":"#### Calculate probabilities","metadata":{}},{"cell_type":"code","source":"# Get frequency of bigrams\nbigram_freq = Counter(bigrams)\ntrigram_freq = Counter(trigrams)\nfourgram_freq = Counter(fourgrams)\n\n# # Calculate probabilities\ntotal_bigrams = sum(bigram_freq.values())\nbigram_probabilities = {gram: freq/total_bigrams for gram, freq in bigram_freq.items()}","metadata":{"execution":{"iopub.status.busy":"2023-11-21T22:39:54.575210Z","iopub.execute_input":"2023-11-21T22:39:54.575660Z","iopub.status.idle":"2023-11-21T22:40:41.320762Z","shell.execute_reply.started":"2023-11-21T22:39:54.575624Z","shell.execute_reply":"2023-11-21T22:40:41.319292Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"#### Get top 10 bigrams","metadata":{}},{"cell_type":"code","source":"top_10_bigrams = sorted(bigram_probabilities.items(), key=lambda item: item[1], reverse=True)[:10]\nprint(\"{:<40} {:<20}\".format(\"\\nTop 10 fourgrams \", \" Probability\"))\n\n# Print each element in the desired format\nfor ngram, probability in top_10_bigrams:\n    ngram_str = ' '.join(ngram)\n    print(\"{:<40} {:<20}\".format(ngram_str, probability))","metadata":{"execution":{"iopub.status.busy":"2023-11-21T22:40:41.322378Z","iopub.execute_input":"2023-11-21T22:40:41.322885Z","iopub.status.idle":"2023-11-21T22:40:45.028829Z","shell.execute_reply.started":"2023-11-21T22:40:41.322844Z","shell.execute_reply":"2023-11-21T22:40:45.027657Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"\nTop 10 fourgrams                         Probability        \nዓ ም                                      0.0018562964337997358\nነገር ግን                                   0.0007995556147818973\nቀን ዓ                                     0.0007798519096186599\nብቻ ሳይሆን                                  0.00041674077161042754\nአዲስ አበባ                                  0.00038229632461454256\nኤ አ                                      0.00037214817571467967\nምክር ቤት                                   0.00037207410163511863\nበአዲስ አበባ                                 0.00036607410119067415\nእ ኤ                                      0.00035088891488066037\nምን ያህል                                   0.0003265926167846383\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 1.2.2. probabilities of trigrams and top 10 most likely trigrams","metadata":{}},{"cell_type":"markdown","source":"#### Calculate probabilities","metadata":{}},{"cell_type":"code","source":"total_trigrams = sum(trigram_freq.values())\ntrigram_probabilities = {gram: freq/total_trigrams for gram, freq in trigram_freq.items()}","metadata":{"execution":{"iopub.status.busy":"2023-11-21T22:40:45.030547Z","iopub.execute_input":"2023-11-21T22:40:45.030885Z","iopub.status.idle":"2023-11-21T22:40:53.866894Z","shell.execute_reply.started":"2023-11-21T22:40:45.030855Z","shell.execute_reply":"2023-11-21T22:40:53.865750Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"#### Get top 10 trigrams","metadata":{}},{"cell_type":"code","source":"top_10_trigrams = sorted(trigram_probabilities.items(), key=lambda item: item[1], reverse=True)[:10]\nprint(\"{:<40} {:<20}\".format(\"\\nTop 10 fourgrams \", \" Probability\"))\n# Print each element in the desired format\nfor ngram, probability in top_10_trigrams:\n    ngram_str = ' '.join(ngram)\n    print(\"{:<40} {:<20}\".format(ngram_str, probability))","metadata":{"execution":{"iopub.status.busy":"2023-11-21T22:40:53.868198Z","iopub.execute_input":"2023-11-21T22:40:53.868568Z","iopub.status.idle":"2023-11-21T22:40:57.616465Z","shell.execute_reply.started":"2023-11-21T22:40:53.868536Z","shell.execute_reply":"2023-11-21T22:40:57.615513Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"\nTop 10 fourgrams                         Probability        \nቀን ዓ ም                                   0.000776148263133076\nእ ኤ አ                                    0.00034837042198080325\nዓ ም ጀምሮ                                  0.00011970372143758836\nተወካዮች ምክር ቤት                             0.00010162964468587329\nአበባ ከተማ አስተዳደር                           8.792593895199096e-05\nየአዲስ አበባ ከተማ                             8.214816031824597e-05\nበዓለም አቀፍ ደረጃ                             6.70370469684514e-05\nከጊዜ ወደ ጊዜ                                6.548149118244314e-05\nጥር ቀን ዓ                                  6.540741709739512e-05\nየካቲት ቀን ዓ                                6.496297258710705e-05\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### 1.2.3. probabilities of 4-grams and top 10 most likely 4-grams","metadata":{}},{"cell_type":"markdown","source":"#### Calculate probabilities","metadata":{}},{"cell_type":"code","source":"total_fourgrams = sum(fourgram_freq.values())\nfourgrams_probabilities = {gram: freq/total_fourgrams for gram, freq in fourgram_freq.items()}","metadata":{"execution":{"iopub.status.busy":"2023-11-21T22:40:57.618616Z","iopub.execute_input":"2023-11-21T22:40:57.618958Z","iopub.status.idle":"2023-11-21T22:41:05.872758Z","shell.execute_reply.started":"2023-11-21T22:40:57.618929Z","shell.execute_reply":"2023-11-21T22:41:05.871509Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"#### Get top 10 fourgrams\n","metadata":{}},{"cell_type":"code","source":"top_10_fourgrams = sorted(fourgrams_probabilities.items(), key=lambda item: item[1], reverse=True)[:10]\nprint(\"{:<40} {:<20}\".format(\"\\nTop 10 fourgrams \", \"  Probability\"))\n\n# Print each element in the desired format\nfor ngram, probability in top_10_fourgrams:\n    ngram_str = ' '.join(ngram)\n    print(\"{:<40} {:<20}\".format(ngram_str, probability))\n","metadata":{"execution":{"iopub.status.busy":"2023-11-21T22:41:05.880874Z","iopub.execute_input":"2023-11-21T22:41:05.881287Z","iopub.status.idle":"2023-11-21T22:41:09.632390Z","shell.execute_reply.started":"2023-11-21T22:41:05.881253Z","shell.execute_reply":"2023-11-21T22:41:09.631224Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"\nTop 10 fourgrams                          Probability       \nጥር ቀን ዓ ም                                6.503705148971514e-05\nየካቲት ቀን ዓ ም                              6.48148292181102e-05\nመጋቢት ቀን ዓ ም                              6.25926065020607e-05\nጥቅምት ቀን ዓ ም                              5.311112291358287e-05\nግንቦት ቀን ዓ ም                              5.222223382716307e-05\nአመለካከት ብቻ የሚያንፀባርቅ መሆኑን                  5.214815973662809e-05\nየጸሐፊውን አመለካከት ብቻ የሚያንፀባርቅ                5.029630747325351e-05\nጽሑፉ የጸሐፊውን አመለካከት ብቻ                     5.0148159292183544e-05\nቀን ዓ ም ጀምሮ                               4.977778883950863e-05\nሰኔ ቀን ዓ ም                                4.851852930041392e-05\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a name=\"1.3\"></a>\n###  1.3. What is the probability of the sentence. \"ኢትዮጵያ ታሪካዊ ሀገር ናት \". ","metadata":{}},{"cell_type":"markdown","source":"<a name=\"99\"></a>\n- [ Top](#99)","metadata":{}},{"cell_type":"code","source":"def generate_ngrams(tokens, n):\n    return list(ngrams(tokens, n))\n\n\n#n-gram_probabilities is a dictionary containing n-gram probabilities\ndef probability_of_sentence(sentence,n):\n    sentence_tokens = sentence.split()\n    ngrams_sentence = generate_ngrams(sentence_tokens, n)\n    sentence_probability = 1.0\n    \n    if n==2:\n        for bg in ngrams_sentence:\n            if bg in bigram_probabilities:\n                sentence_probability *= bigram_probabilities[bg]\n            else:\n                sentence_probability *= 0.0  # Assuming we assign zero probability for unseen fourgrams\n        return sentence_probability\n    elif n==3:\n        for bg in ngrams_sentence:\n            if bg in trigram_probabilities:\n                sentence_probability *= trigram_probabilities[bg]\n            else:\n                sentence_probability *= 0.0  # Assuming we assign zero probability for unseen fourgrams\n        return sentence_probability\n    elif n==4:\n        for bg in ngrams_sentence:\n            if bg in fourgrams_probabilities:\n                sentence_probability *= fourgrams_probabilities[bg]\n            else:\n                sentence_probability *= 0.0  # Assuming we assign zero probability for unseen fourgrams\n        return sentence_probability\n    else:\n        return 'Please input n 2, 3, or 4'\n    \nsentence1 = \"ኢትዮጵያ ታሪካዊ ሀገር ናት\"\n\n\n\nprint(f\"\\n\\n\\nThe probability of the sentence(ኢትዮጵያ ታሪካዊ ሀገር ናት) using bigram is:    {probability_of_sentence(sentence1, 2)}\")\n\nprint(f\"\\nThe probability of the sentence(ኢትዮጵያ ታሪካዊ ሀገር ናት) using trigram is:    {probability_of_sentence(sentence1, 3)}\")\n\nprint(f\"\\nThe probability of the sentence(ኢትዮጵያ ታሪካዊ ሀገር ናት) using fourgram is:    {probability_of_sentence(sentence1, 4)}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-11-21T22:41:09.633581Z","iopub.execute_input":"2023-11-21T22:41:09.633888Z","iopub.status.idle":"2023-11-21T22:41:09.645698Z","shell.execute_reply.started":"2023-11-21T22:41:09.633860Z","shell.execute_reply":"2023-11-21T22:41:09.644615Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"\n\n\nThe probability of the sentence(ኢትዮጵያ ታሪካዊ ሀገር ናት) using bigram is:    4.3895757354063326e-20\n\nThe probability of the sentence(ኢትዮጵያ ታሪካዊ ሀገር ናት) using trigram is:    5.486970075700204e-15\n\nThe probability of the sentence(ኢትዮጵያ ታሪካዊ ሀገር ናት) using fourgram is:    7.407409053498309e-08\n","output_type":"stream"}]},{"cell_type":"markdown","source":"`Bigram Model (n=2)`: The probability is incredibly low at approximately 4.39e-20. This suggests that, according to the bigram model, the chance of this exact sequence of words appearing together is extremely rare within the context of the text corpus used for the model.\n\n`Trigram Model (n=3)`: The probability increases significantly to around 5.49e-15 when using a trigram model. This implies that considering a sequence of three words at a time provides a higher likelihood for this sentence to be observed, reflecting a more common occurrence in the source material.\n\n`Fourgram Model (n=4)`: The probability surges to approximately 7.41e-08 when evaluated with a fourgram model. This indicates that the sequence of four words is much more common, and the sentence is much more likely to occur in the dataset that was used to train the model.","metadata":{}},{"cell_type":"markdown","source":"The increasing probabilities of the Amharic sentence \"`ኢትዮጵያ ታሪካዊ ሀገር ናት`\" with larger n-grams from bigrams to fourgrams demonstrate the importance of context in language modeling. The low probability with the bigram model indicates a rare occurrence of word pairs, while the higher probabilities with trigram and fourgram models suggest that longer word sequences from the sentence are more common in the dataset. This trend highlights the need for extensive and context-rich datasets to train accurate language models, especially for languages with complex structures like Amharic.","metadata":{}},{"cell_type":"markdown","source":"### a) using Bigrams","metadata":{}},{"cell_type":"code","source":"sentence1 = \"ኢትዮጵያ ታሪካዊ ሀገር ናት\"\nsentence2 = \"በፈታኝ ሁኔታ ውስጥ ማለፍ\"\nsentence3 = 'ተወካዮች ምክር ቤት'\nsentence4 = 'በዓለም አቀፍ ደረጃ'\nsentence5 = 'በአዲስ አበባ'\n\n\nprint(f\"\\n\\n\\nThe probability of the sentence(ኢትዮጵያ ታሪካዊ ሀገር ናት) is: \\\n{probability_of_sentence(sentence1, 2)}\")\nprint(f\"\\nThe probability of the sentence(በፈታኝ ሁኔታ ውስጥ ማለፍ) is:  \\\n{probability_of_sentence(sentence2, 2)}\")\nprint(f\"\\nThe probability of the sentence(ተወካዮች ምክር ቤት) is:   \\\n{probability_of_sentence(sentence3, 2)}\")\nprint(f\"\\nThe probability of the sentence(በዓለም አቀፍ ደረጃ) is:   \\\n{probability_of_sentence(sentence4, 2)}\")\nprint(f\"\\nThe probability of the sentence(በአዲስ አበባ) is:   \\\n{probability_of_sentence(sentence5, 2)}\")","metadata":{"execution":{"iopub.status.busy":"2023-11-21T22:41:09.647249Z","iopub.execute_input":"2023-11-21T22:41:09.647698Z","iopub.status.idle":"2023-11-21T22:41:09.665219Z","shell.execute_reply.started":"2023-11-21T22:41:09.647659Z","shell.execute_reply":"2023-11-21T22:41:09.664047Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"\n\n\nThe probability of the sentence(ኢትዮጵያ ታሪካዊ ሀገር ናት) is: 4.3895757354063326e-20\n\nThe probability of the sentence(በፈታኝ ሁኔታ ውስጥ ማለፍ) is:  1.0239904729428439e-17\n\nThe probability of the sentence(ተወካዮች ምክር ቤት) is:   4.169986351657763e-08\n\nThe probability of the sentence(በዓለም አቀፍ ደረጃ) is:   1.7296891451391234e-08\n\nThe probability of the sentence(በአዲስ አበባ) is:   0.00036607410119067415\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### b) Using trigrams","metadata":{}},{"cell_type":"code","source":"sentence1 = \"ኢትዮጵያ ታሪካዊ ሀገር ናት\"\nsentence2 = \"በፈታኝ ሁኔታ ውስጥ ማለፍ\"\nsentence3 = 'ተወካዮች ምክር ቤት'\nsentence4 = 'በዓለም አቀፍ ደረጃ'\n\n\nprint(f\"\\n\\nThe probability of the sentence(ኢትዮጵያ ታሪካዊ ሀገር ናት) is: \\\n{probability_of_sentence(sentence1, 3)}\")\nprint(f\"\\nThe probability of the sentence(በፈታኝ ሁኔታ ውስጥ ማለፍ) is:  \\\n{probability_of_sentence(sentence2, 3)}\")\n\nprint(f\"\\nThe probability of the sentence(ተወካዮች ምክር ቤት) is:   \\\n{probability_of_sentence(sentence3, 3)}\")\nprint(f\"\\nThe probability of the sentence(በዓለም አቀፍ ደረጃ) is:   \\\n{probability_of_sentence(sentence4, 3)}\")","metadata":{"execution":{"iopub.status.busy":"2023-11-21T22:41:09.666430Z","iopub.execute_input":"2023-11-21T22:41:09.666770Z","iopub.status.idle":"2023-11-21T22:41:09.682828Z","shell.execute_reply.started":"2023-11-21T22:41:09.666741Z","shell.execute_reply":"2023-11-21T22:41:09.681682Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"\n\nThe probability of the sentence(ኢትዮጵያ ታሪካዊ ሀገር ናት) is: 5.486970075700204e-15\n\nThe probability of the sentence(በፈታኝ ሁኔታ ውስጥ ማለፍ) is:  0.0\n\nThe probability of the sentence(ተወካዮች ምክር ቤት) is:   0.00010162964468587329\n\nThe probability of the sentence(በዓለም አቀፍ ደረጃ) is:   6.70370469684514e-05\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### c) Using fourgrams","metadata":{}},{"cell_type":"code","source":"\nsentence1 = \"ኢትዮጵያ ታሪካዊ ሀገር ናት\"\nsentence2 = \"ግለሰቦች በፈታኝ ሁኔታ ውስጥ በማለፍ ብቃታቸውን የሚያስመሰክሩበት መሆኑን\"  \nsentence3 = \"በፈታኝ ሁኔታ ውስጥ በማለፍ\"\n\n\nprint(f\"\\n\\nThe probability of the sentence(ኢትዮጵያ ታሪካዊ ሀገር ናት) is:   \\\n{probability_of_sentence(sentence1, 4)}\")\nprint(f\"\\nThe probability of the sentence(ግለሰቦች በፈታኝ ሁኔታ ውስጥ በማለፍ ብቃታቸውን የሚያስመሰክሩበት መሆኑን) is:  \\\n{probability_of_sentence(sentence2, 4)}\")\n\nprint(f\"\\nThe probability of the sentence(በፈታኝ ሁኔታ ውስጥ በማለፍ) is:   \\\n{probability_of_sentence(sentence3, 4)}\")","metadata":{"execution":{"iopub.status.busy":"2023-11-21T22:41:09.684555Z","iopub.execute_input":"2023-11-21T22:41:09.685000Z","iopub.status.idle":"2023-11-21T22:41:09.697323Z","shell.execute_reply.started":"2023-11-21T22:41:09.684959Z","shell.execute_reply":"2023-11-21T22:41:09.696126Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"\n\nThe probability of the sentence(ኢትዮጵያ ታሪካዊ ሀገር ናት) is:   7.407409053498309e-08\n\nThe probability of the sentence(ግለሰቦች በፈታኝ ሁኔታ ውስጥ በማለፍ ብቃታቸውን የሚያስመሰክሩበት መሆኑን) is:  2.2301374979696553e-36\n\nThe probability of the sentence(በፈታኝ ሁኔታ ውስጥ በማለፍ) is:   7.407409053498309e-08\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a name=\"1.4\"></a>\n###  1.4. Generate random sentences using n-grams; explain what happens as n-increases based on your output","metadata":{"execution":{"iopub.status.busy":"2023-11-11T22:01:14.733534Z","iopub.execute_input":"2023-11-11T22:01:14.734782Z","iopub.status.idle":"2023-11-11T22:01:14.744000Z","shell.execute_reply.started":"2023-11-11T22:01:14.734703Z","shell.execute_reply":"2023-11-11T22:01:14.742150Z"}}},{"cell_type":"markdown","source":"<a name=\"99\"></a>\n- [ Top](#99)","metadata":{}},{"cell_type":"markdown","source":"### Define random sentence generator function ","metadata":{}},{"cell_type":"code","source":"def sentence_generator(gram_freq,n, token, num_words=8):\n    if n==2:\n        bigram_sentence = [token]\n        for _ in range(num_words - 1):\n            next_tokens = [pair[1] for pair in gram_freq if pair[0] == token]\n            if not next_tokens:\n                break\n            token = random.choice(next_tokens)\n            bigram_sentence.append(token)\n        return ' '.join(bigram_sentence)\n    elif n==3:\n        trigram_sentence = [token]\n        for _ in range(num_words - 1):\n            next_tokens = [pair[1] for pair in gram_freq if pair[0] == token]\n            if not next_tokens:\n                break\n            token = random.choice(next_tokens)\n            trigram_sentence.append(token)\n        return ' '.join(trigram_sentence)\n    elif n==4:\n        fourgram_sentence = [token]\n        for _ in range(num_words - 1):\n            next_tokens = [pair[1] for pair in gram_freq if pair[0] == token]\n            if not next_tokens:\n                break\n            token = random.choice(next_tokens)\n            fourgram_sentence.append(token)\n        return ' '.join(fourgram_sentence)\n    else:\n        return 'Please input one of the following n values n=2,n=3, n=4'\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2023-11-21T22:41:09.698879Z","iopub.execute_input":"2023-11-21T22:41:09.699535Z","iopub.status.idle":"2023-11-21T22:41:09.713073Z","shell.execute_reply.started":"2023-11-21T22:41:09.699503Z","shell.execute_reply":"2023-11-21T22:41:09.712283Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"### a) Using bigrams","metadata":{}},{"cell_type":"code","source":"import random\n# def generate_sentence(bigram_freq, token, num_words=8):\n#     sentence = [token]\n#     for _ in range(num_words - 1):\n#         next_tokens = [pair[1] for pair in bigram_freq if pair[0] == token]\n#         if not next_tokens:\n#             break\n#         token = random.choice(next_tokens)\n#         sentence.append(token)\n#     return ' '.join(sentence)\n     \n# Generate a sentence starting with a given word\nstarting_word1 = 'ኢትዮጵያ'\nstarting_word2 = 'በተደጋጋሚ'\nstarting_word3 = 'ከተለያዩ'\nstarting_word4 = 'የአለም'\nstarting_word5 = 'በደቡብ'\nstarting_word6 = 'የአፍሪካ'\nstarting_word7 = 'በሩቅ'\n\n\n\nprint('ኢትዮጵያ as a starting word =  ',sentence_generator(bigram_freq,2, starting_word1))\nprint('በተደጋጋሚ as a starting word =  ',sentence_generator(bigram_freq,2,starting_word2))\nprint('ከተለያዩ as a starting word = ',sentence_generator(bigram_freq,2,starting_word3))\nprint('የአለም as a starting word = ',sentence_generator(bigram_freq,2,starting_word4))\nprint('በደቡብ as a starting word =  ',sentence_generator(bigram_freq,2,starting_word5))\nprint('የአፍሪካ as a starting word = ',sentence_generator(bigram_freq,2,starting_word6))\nprint('በሩቅ as a starting word = ',sentence_generator(bigram_freq,2,starting_word7))","metadata":{"execution":{"iopub.status.busy":"2023-11-21T22:41:09.714597Z","iopub.execute_input":"2023-11-21T22:41:09.715152Z","iopub.status.idle":"2023-11-21T22:41:41.377556Z","shell.execute_reply.started":"2023-11-21T22:41:09.715122Z","shell.execute_reply":"2023-11-21T22:41:41.376242Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"ኢትዮጵያ as a starting word =   ኢትዮጵያ ውሥጥ ጣሊያኖች የቅኝ ድንበር መመለስ የፈለግን እኔ\nበተደጋጋሚ as a starting word =   በተደጋጋሚ የሚቋቋሙበትና መደበኛ ጉባኤው በቀረበው ምርት ኦፕሬሽን ጁባ\nከተለያዩ as a starting word =  ከተለያዩ ምግቦች ውስጥ አድናቆት እነዛኞቹ ደግሞ ምናለበት ይህንን\nየአለም as a starting word =  የአለም ቅርሶችን ግእዝ የኢትዮጵያውያን ባህልና ልማድ ከተጨባጩ የኢኮኖሚ\nበደቡብ as a starting word =   በደቡብ አሜሪካዋ ቬኒዙዌላ ወይም የእጩ ፕሬዚዳንት አንዳንድ የብረት\nየአፍሪካ as a starting word =  የአፍሪካ ቀዳሚ መመዘኛ ተደርጎ አሸናፊውን አለቀ ስለዚህ እንደተጠቃሚው\nበሩቅ as a starting word =  በሩቅ ሰምተን በልቦና ማላመጥ አዝማሪውንም ውጦ ቅምም ወጨፎ\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### b) Using trigrams","metadata":{}},{"cell_type":"code","source":"# Generate a sentence starting with a given word\nstarting_word1 = 'ኢትዮጵያ'\nstarting_word2 = 'በተደጋጋሚ'\nstarting_word3 = 'ከተለያዩ'\nstarting_word4 = 'የአለም'\nstarting_word5 = 'በደቡብ'\nstarting_word6 = 'የአፍሪካ'\nstarting_word7 = 'በሩቅ'\n\n\n\nprint('ኢትዮጵያ as a starting word =  ',sentence_generator(trigram_freq,3, starting_word1))\nprint('በተደጋጋሚ as a starting word =  ',sentence_generator(trigram_freq,3, starting_word2))\nprint('ከተለያዩ as a starting word = ',sentence_generator(trigram_freq,3, starting_word3))\nprint('የአለም as a starting word = ',sentence_generator(trigram_freq,3, starting_word4))\nprint('በደቡብ as a starting word =  ',sentence_generator(trigram_freq,3, starting_word5))\nprint('የአፍሪካ as a starting word = ',sentence_generator(trigram_freq,3, starting_word6))\nprint('በሩቅ as a starting word = ',sentence_generator(trigram_freq,3, starting_word7))","metadata":{"execution":{"iopub.status.busy":"2023-11-21T22:41:41.378986Z","iopub.execute_input":"2023-11-21T22:41:41.380063Z","iopub.status.idle":"2023-11-21T22:42:26.378682Z","shell.execute_reply.started":"2023-11-21T22:41:41.380025Z","shell.execute_reply":"2023-11-21T22:42:26.377552Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"ኢትዮጵያ as a starting word =   ኢትዮጵያ ከመምጣታችን ዘግየት ብሎ ታኬታ ዋጋ ደግሞ በአሣታሚዎችም\nበተደጋጋሚ as a starting word =   በተደጋጋሚ ወረራ ሳቢያ የወሰዱትን በሚሊዮን የሚቆጠር መሆኑን ብናውቅም\nከተለያዩ as a starting word =  ከተለያዩ አካባቢዎች ሦስት ከንጋቱ ሰዓት ላይ ማሻሻያ የሚጨምር\nየአለም as a starting word =  የአለም ጦርነት ያህል ተበድላ እንደኖረች በመጽሐፉ ውስጥ መግባት\nበደቡብ as a starting word =   በደቡብ ዐረብ ነን ከሚሉት ውስጥ ሆነን ቀረን ስልሳ\nየአፍሪካ as a starting word =  የአፍሪካ አገሮች ያሏቸውን ጉዳዮች እውቅና መስጠት ሁሉም ባለሥልጣናት\nበሩቅ as a starting word =  በሩቅ በሆያ ሆዬ አዳም በመሆኑ አንድን የደርግ ወታደሮችና\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### c) Using fourgrams","metadata":{}},{"cell_type":"code","source":"starting_word1 = 'ኢትዮጵያ'\nstarting_word2 = 'በተደጋጋሚ'\nstarting_word3 = 'ከተለያዩ'\nstarting_word4 = 'የአለም'\nstarting_word5 = 'በደቡብ'\nstarting_word6 = 'የአፍሪካ'\nstarting_word7 = 'በሩቅ'\n\n\n\nprint('ኢትዮጵያ as a starting word =  ',sentence_generator(fourgram_freq,4, starting_word1))\nprint('በተደጋጋሚ as a starting word =  ',sentence_generator(fourgram_freq,4, starting_word2))\nprint('ከተለያዩ as a starting word = ',sentence_generator(fourgram_freq,4, starting_word3))\nprint('የአለም as a starting word = ',sentence_generator(fourgram_freq,4, starting_word4))\nprint('በደቡብ as a starting word =  ',sentence_generator(fourgram_freq,4, starting_word5))\nprint('የአፍሪካ as a starting word = ',sentence_generator(fourgram_freq,4, starting_word6))\nprint('በሩቅ as a starting word = ',sentence_generator(fourgram_freq,4, starting_word7))","metadata":{"execution":{"iopub.status.busy":"2023-11-21T22:42:26.380156Z","iopub.execute_input":"2023-11-21T22:42:26.380816Z","iopub.status.idle":"2023-11-21T22:43:13.626335Z","shell.execute_reply.started":"2023-11-21T22:42:26.380783Z","shell.execute_reply":"2023-11-21T22:43:13.625117Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"ኢትዮጵያ as a starting word =   ኢትዮጵያ ቀሚስ በጭንቅላቱ የሚመላለሰው ምንድነው ያለው ዕርቅ እንዲኖር\nበተደጋጋሚ as a starting word =   በተደጋጋሚ ተመሳሳይ አቋም ከያዙ ወዲህ በቲማቲም ቆርቆሮ በቆርቆሮ\nከተለያዩ as a starting word =  ከተለያዩ የፍትሕ ቀን እሱ ላይ የላቀ ድል በሁለተኛው\nየአለም as a starting word =  የአለም ታላላቅ ኢትዮጵያውያን ወንድሞቻቸው ጎን ለጎን ዓለማዊ ዝግጅታቸው\nበደቡብ as a starting word =   በደቡብ አረቢያ እየተባረሩ ይህ ዞን ቀበሌዎች ውስጥ ያሉ\nየአፍሪካ as a starting word =  የአፍሪካ አየር መንገዱ ግራና ቀኝ ሲኦልና የአፍሪካ አገሮች\nበሩቅ as a starting word =  በሩቅ ላለችው አባወራ ሦስት ዓመታት መልካም አስተዳደራችን አንባገነናዊ\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a name=\"2\"></a>\n### 2. Evaluate these Language Models Using Intrinsic Evaluation Method.","metadata":{}},{"cell_type":"markdown","source":"<a name=\"perplexity\"></a>\n### Perplexity\n\nIn order to implement the perplexity formula, you'll need to know how to implement m-th order root of a variable.\n\n\\begin{equation*}\nPP(W)=\\sqrt[M]{\\prod_{i=1}^{m}{\\frac{1}{P(w_i|w_{i-1})}}}\n\\end{equation*}\n\nFrom calculus:\n\n\\begin{equation*}\n\\sqrt[M]{\\frac{1}{x}} = x^{-\\frac{1}{M}}\n\\end{equation*}\n\nHere is bellow a code that will help us with the formula.","metadata":{}},{"cell_type":"markdown","source":"<a name=\"99\"></a>\n- [ Top](#99)","metadata":{}},{"cell_type":"code","source":"import math\nfrom collections import Counter\n\ndef calculate_perplexity(ngrams, total_words):\n    model_entropy = 0.0\n    for ngram, count in ngrams.items():\n        prob = count / total_words\n        model_entropy += -math.log2(prob)\n\n    model_entropy /= total_words\n    return math.pow(2, model_entropy)\n\ndef evaluate_model(n, train_tokens, test_tokens):\n\n    ngrams_train = Counter(tuple(train_tokens[i:i + n]) for i in range(len(train_tokens) - n + 1))\n    total_words_train = len(train_tokens)\n\n    ngrams_test = Counter(tuple(test_tokens[i:i + n]) for i in range(len(test_tokens) - n + 1))\n    total_words_test = len(test_tokens)\n\n    return calculate_perplexity(ngrams_test, total_words_test)\n# Load the tokens and calculate frequencies\nwith open('/kaggle/input/saved-tokens/tokens_cleaned(1).pkl', 'rb') as file:\n    tokens = pickle.load(file)\n# Split the data into training and test sets (80% training, 20% test)\ntrain_size = int(0.8 * len(tokens))\ntrain_tokens = tokens[:train_size]\ntest_tokens = tokens[train_size:]\n\n# Evaluating n-gram models for n = 1, 2, 3, 4\nperplexities = []\nfor n in range(1, 5):\n    perplexity = evaluate_model(n, train_tokens, test_tokens)\n    perplexities.append(perplexity)\n\n\n# Storing the results in a list of tuples\nresults = []\n\nfor n, perplexity in enumerate(perplexities):\n    results.append((n + 1, perplexity))\n\n# Printing the results\nfor n, perplexity in results:\n    print(f\"Perplexity of {n}-gram model is  : {perplexity}\")\n","metadata":{"execution":{"iopub.status.busy":"2023-11-21T22:43:13.627772Z","iopub.execute_input":"2023-11-21T22:43:13.628195Z","iopub.status.idle":"2023-11-21T22:44:17.093935Z","shell.execute_reply.started":"2023-11-21T22:43:13.628166Z","shell.execute_reply":"2023-11-21T22:44:17.092934Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"Perplexity of 1-gram model is  : 3.4376775176273546\nPerplexity of 2-gram model is  : 13141.287652919986\nPerplexity of 3-gram model is  : 651342.603025604\nPerplexity of 4-gram model is  : 1549724.6862948143\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The perplexity values you obtained indicate how well our n-gram models generalize to unseen data. Perplexity is a measure of uncertainty or \"surprise\" associated with predicting the next word in a sequence. A lower perplexity indicates better performance.\n\nHere's a brief interpretation of the results:\n\n`Unigram (n=1):`\n\n>Perplexity: 3.44\nThe unigram model has relatively low perplexity, suggesting that it provides a good estimate of word probabilities based on individual words. This is expected, as unigrams consider each word in isolation.\n\n`Bigram (n=2):`\n>Perplexity: 13141.29\nThe bigram model has a significantly higher perplexity compared to the unigram model. This indicates that predicting the next word based on the previous word alone might not be as accurate. It suggests that the model struggles with capturing dependencies between adjacent words.\n\n`Trigram (n=3):`\n>Perplexity: 651342.60\nThe trigram model has an even higher perplexity, indicating that considering the previous two words for prediction does not lead to effective generalization. The model might be overfitting to the training data and failing to capture broader linguistic patterns.\n\n`Fourgram (n=4):`\n>Perplexity: 1549724.69`\nThe fourgram model exhibits the highest perplexity, suggesting that considering the previous three words for prediction may lead to overfitting or insufficient data for effective generalization. The model might struggle with unseen combinations of four words.\n\nDescription:\nThe results highlight the trade-off between model complexity and generalization. While the unigram model performs well, higher-order n-gram models (bigram, trigram, fourgram) face challenges in capturing dependencies and suffer from increasing perplexity, indicating potential overfitting to the training data. This could be due to the limitations of n-gram models in handling the complexity of natural language, especially when dealing with longer dependencies between words. Consideration of more advanced language models or techniques may be warranted for improved performance on this task.","metadata":{}},{"cell_type":"markdown","source":"<a name=\"2\"></a>\n### 3. Evaluate these Language Models Using Extrinsic Evaluation Method","metadata":{}},{"cell_type":"markdown","source":"<a name=\"99\"></a>\n- [ Top](#99)","metadata":{}},{"cell_type":"markdown","source":"### Create predictive model for extrinsic evaluation","metadata":{}},{"cell_type":"code","source":"# Function for predictive text entry (for extrinsic evaluation)\ndef predict_next_word(current_sentence, bigram_freq):\n    current_tokens = current_sentence.split()\n    last_token = current_tokens[-1]\n    next_words = {pair[1]: freq for pair, freq in bigram_freq.items() if pair[0] == last_token}\n    next_word = max(next_words, key=next_words.get) if next_words else None\n    return next_word","metadata":{"execution":{"iopub.status.busy":"2023-11-21T22:44:17.095415Z","iopub.execute_input":"2023-11-21T22:44:17.096439Z","iopub.status.idle":"2023-11-21T22:44:17.102697Z","shell.execute_reply.started":"2023-11-21T22:44:17.096404Z","shell.execute_reply":"2023-11-21T22:44:17.101413Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"markdown","source":"#### Extrinsic evaluation - predict next word","metadata":{}},{"cell_type":"code","source":"# Extrinsic evaluation - predict next word\ncurrent_sentence = \"ታሪካዊ ሀገር\"\nnext_word = predict_next_word(current_sentence, bigram_freq)\nprint('\\n\\nThe previous words are:  ',current_sentence)\nprint(f'\\n and the suggested word is: {next_word}')","metadata":{"execution":{"iopub.status.busy":"2023-11-21T22:44:17.103949Z","iopub.execute_input":"2023-11-21T22:44:17.104252Z","iopub.status.idle":"2023-11-21T22:44:18.030156Z","shell.execute_reply.started":"2023-11-21T22:44:17.104225Z","shell.execute_reply":"2023-11-21T22:44:18.028910Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"\n\nThe previous words are:   ታሪካዊ ሀገር\n\n and the suggested word is: ውስጥ\n","output_type":"stream"}]},{"cell_type":"code","source":"current_sentence = \"ታሪካዊ\"\nnext_word = predict_next_word(current_sentence, bigram_freq)\nprint('\\n\\nThe previous words are:  ',current_sentence)\nprint(f'\\n and the suggested word is: {next_word}')","metadata":{"execution":{"iopub.status.busy":"2023-11-21T22:44:18.031692Z","iopub.execute_input":"2023-11-21T22:44:18.032123Z","iopub.status.idle":"2023-11-21T22:44:18.952955Z","shell.execute_reply.started":"2023-11-21T22:44:18.032082Z","shell.execute_reply":"2023-11-21T22:44:18.951794Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"\n\nThe previous words are:   ታሪካዊ\n\n and the suggested word is: ዳራ\n","output_type":"stream"}]},{"cell_type":"code","source":"current_sentence = \"በፈታኝ ሁኔታ\"\nnext_word = predict_next_word(current_sentence, bigram_freq)\nprint('\\n\\nThe previous words are:  ',current_sentence)\nprint(f'\\n and the suggested word is: {next_word}')","metadata":{"execution":{"iopub.status.busy":"2023-11-21T22:44:18.954286Z","iopub.execute_input":"2023-11-21T22:44:18.954627Z","iopub.status.idle":"2023-11-21T22:44:19.873857Z","shell.execute_reply.started":"2023-11-21T22:44:18.954596Z","shell.execute_reply":"2023-11-21T22:44:19.872533Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"\n\nThe previous words are:   በፈታኝ ሁኔታ\n\n and the suggested word is: ውስጥ\n","output_type":"stream"}]},{"cell_type":"code","source":"current_sentence = \"ውስጥ በማለፍ\"\nnext_word = predict_next_word(current_sentence, bigram_freq)\nprint('\\n\\nThe previous words are:  ',current_sentence)\nprint(f'\\n and the suggested word is: {next_word}')","metadata":{"execution":{"iopub.status.busy":"2023-11-21T22:44:19.875369Z","iopub.execute_input":"2023-11-21T22:44:19.875781Z","iopub.status.idle":"2023-11-21T22:44:20.838351Z","shell.execute_reply.started":"2023-11-21T22:44:19.875749Z","shell.execute_reply":"2023-11-21T22:44:20.837209Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"\n\nThe previous words are:   ውስጥ በማለፍ\n\n and the suggested word is: ወደ\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## `Conclusion`\nThe model utilizes bigram frequencies to predict the next word in a sentence, and the given test cases show that it can suggest contextually reasonable continuations. For instance, from the historical context indicated by \"ታሪካዊ ሀገር,\" the model aptly predicts \"ውስጥ,\" reflecting a coherent phrase structure in the language being tested. The other test cases align similarly with the expected linguistic patterns. Thus, based on these examples, we can infer that the model is fairly effective in its predictive capability, indicating a good understanding of word pair probabilities within the corpus from which the bigram frequencies were derived.","metadata":{}}]}